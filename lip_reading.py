# -*- coding: utf-8 -*-
"""Lip Reading

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uiBFrYDXYXZIehOEfGxyDlKTFF0e6lws
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install av
!pip install tqdm
!pip install SpeechRecognition
!pip install jiwer
!pip install keras_cv

import speech_recognition as sr

recognizer = sr.Recognizer()

"""**CONCATENATING FILES IN DATASET**"""

!cat lrw-v1* > lrw-v1.tar

!cat lrs2_v1_parta* > lrs2_v1.tar

"""**EXTRACTING THE FILES FROM DATASET**


"""

!tar -xvf lrs2_v1.tar --exclude=mvlrs_v1/main --skip-old-files --remove-files

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !tar -xvf lrw-v1.tar --skip-old-files --remove-files
#

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/LRS

!unzip -n LRS2_landmarks.zip

"""**FACE DETECTION (LRS2)**"""

# Commented out IPython magic to ensure Python compatibility.
!wget https://www.doc.ic.ac.uk/~pm4115/tracker/face_detection.zip -O ./face_detection.zip
!unzip -o ./face_detection.zip -d ./
# %cd face_detection
!pip install -e .
# %cd ..

"""**FACE ALIGNMENT(LRS2)**"""

# Commented out IPython magic to ensure Python compatibility.
!wget https://www.doc.ic.ac.uk/~pm4115/tracker/face_alignment.zip -O ./face_alignment.zip
!unzip -o ./face_alignment.zip -d ./
# %cd face_alignment
!pip install -e .
# %cd ..

"""**CREATING WORD CLIPS FOR VISUAL MODULE PRETRAINING**"""

import os
from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip
import subprocess
import os

def ffmpeg_cut_video(video_path, start_time, end_time, output_video_path):
    """
    Uses ffmpeg directly to cut a video segment.
    """
    try:
        cmd = [
            'ffmpeg',
            '-y',  # Overwrites the output file if it exists
            '-i', video_path,  # Input file path
            '-ss', str(start_time),  # Start time
            '-to', str(end_time),  # End time
            '-vf', f'fps30',
            '-c', 'copy',  # Use the same codec for the output file
            '-an',
            output_video_path  # Output file path
        ]
        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Failed to cut video segment: {e}")

def segment_video(video_path, transcript_path, output_dir, base_filename):
    processing_started = False
    counter = 1  # Initialize the counter before its use in the loop

    with open(transcript_path, 'r') as transcript_file:
        for line in transcript_file:
            if "WORD START END ASDSCORE" in line:
                processing_started = True
                continue

            if not processing_started:
                continue

            parts = line.strip().split()
            # Adjust the condition based on your actual data structure
            if len(parts) < 4:
                continue

            # Assuming the format "WORD START END ASDSCORE", adjust as needed
            word, start_time, end_time = parts[0], float(parts[1]), float(parts[2])

            output_video_name = f"{base_filename}_{counter:03d}.mp4"
            output_video_path = os.path.join(output_dir, output_video_name)
            output_txt_name = f"{base_filename}_{counter:03d}.txt"
            output_txt_path = os.path.join(output_dir, output_txt_name)

            ffmpeg_cut_video(video_path, start_time, end_time, output_video_path)

            with open(output_txt_path, 'w') as text_file:
                text_file.write(word + '\n')

            print(f"Processed segment {counter} for {base_filename}: {word}")
            counter += 1  # Ensure the counter is incremented within the loop




def process_directory(input_base_path, output_base_path):
    for root, dirs, files in os.walk(input_base_path):
        for file in files:
            if file.endswith(".txt"):
                base_filename = file.replace('.txt', '')
                video_name = f"{base_filename}.mp4"
                video_path = os.path.join(root, video_name)
                transcript_path = os.path.join(root, file)

                # Determine output paths
                relative_path = os.path.relpath(root, input_base_path)
                output_dir = os.path.join(output_base_path, relative_path)
                os.makedirs(output_dir, exist_ok=True)

                if not os.path.exists(video_path):
                    print(f"Video file {video_path} does not exist.")
                    continue

                # Segment the video
                segment_video(video_path, transcript_path, output_dir, base_filename)

                print(f"Finished processing segments for {video_name}")

def segment_videos(input_base_path, output_base_path):
    subdirs = ['pretrain']
    for subdir in subdirs:
        subdir_path = os.path.join(input_base_path, subdir)
        output_subdir_path = os.path.join(output_base_path, subdir)
        os.makedirs(output_subdir_path, exist_ok=True)  # Ensure output subdirectory exists
        process_directory(subdir_path, output_subdir_path)

# Use the function like this
input_base_path = '/content/drive/MyDrive/LRS/mvlrs_v1'  # Replace with the actual base path to your 'lrs2' folder
output_base_path = '/content/drive/MyDrive/LRS/word_clips'  # Replace with the desired output base path
segment_videos(input_base_path, output_base_path)

!pip install tqdm
!pip install opencv-python
!pip install ffmpeg-python
!pip install sentencepiece
!pip install scikit-image

"""**LRS2 PREPROCESSING**

"""

import argparse
import glob
import math
import os
import pickle
import shutil
import warnings

import cv2
import numpy as np
from face_alignment.ibug.face_alignment import FANPredictor
from face_detection.ibug.face_detection import RetinaFacePredictor
import ffmpeg
#from data.data_module import AVSRDataLoader
from tqdm import tqdm

#!pip install sentencepiece
import os
import random

import sentencepiece
import torch
import torchaudio
import torchvision




SP_MODEL_PATH = '/content/drive/MyDrive/LRS/unigram5000.model'


DICT_PATH = '/content/drive/MyDrive/LRS/unigram5000_units.txt'



class FunctionalModule(torch.nn.Module):
    def __init__(self, functional):
        super().__init__()
        self.functional = functional

    def forward(self, input):
        return self.functional(input)


class AdaptiveTimeMask(torch.nn.Module):
    def __init__(self, window, stride):
        super().__init__()
        self.window = window
        self.stride = stride

    def forward(self, x):
        # x: [T, ...]
        cloned = x.clone()
        length = cloned.size(0)
        n_mask = int((length + self.stride - 0.1) // self.stride)
        ts = torch.randint(0, self.window, size=(n_mask, 2))
        for t, t_end in ts:
            if length - t <= 0:
                continue
            t_start = random.randrange(0, length - t)
            if t_start == t_start + t:
                continue
            t_end += t_start
            cloned[t_start:t_end] = 0
        return cloned

def create_tfrecord_example(video_frames):
    example = tf.train.Example(features=tf.train.Features(feature={
        'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[video_frames.shape[1]])),
        'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[video_frames.shape[2]])),
        'frames': tf.train.Feature(
            bytes_list=tf.train.BytesList(value=[video_frames.tostring()])
        ),
    }))
    return example

'''class AddNoise(torch.nn.Module):
    def __init__(
        self,
        noise_filename=NOISE_FILENAME,
        snr_target=None,
    ):
        super().__init__()
        self.snr_levels = [snr_target] if snr_target else [-5, 0, 5, 10, 15, 20, 999999]
        self.noise, sample_rate = torchaudio.load(noise_filename)
        assert sample_rate == 16000

    def forward(self, speech):
        # speech: T x 1
        # return: T x 1
        speech = speech.t()
        start_idx = random.randint(0, self.noise.shape[1] - speech.shape[1])
        noise_segment = self.noise[:, start_idx : start_idx + speech.shape[1]]
        snr_level = torch.tensor([random.choice(self.snr_levels)])
        noisy_speech = torchaudio.functional.add_noise(speech, noise_segment, snr_level)
        return noisy_speech.t()


class VideoTransform:
    def __init__(self, subset):
        if subset == "train":
            self.video_pipeline = torch.nn.Sequential(
                FunctionalModule(lambda x: x / 255.0),
                torchvision.transforms.RandomCrop(88),
                torchvision.transforms.Grayscale(),
                AdaptiveTimeMask(10, 25),
                torchvision.transforms.Normalize(0.421, 0.165),
            )
        elif subset == "val" or subset == "test":
            self.video_pipeline = torch.nn.Sequential(
                FunctionalModule(lambda x: x / 255.0),
                torchvision.transforms.CenterCrop(88),
                torchvision.transforms.Grayscale(),
                torchvision.transforms.Normalize(0.421, 0.165),
            )

    def __call__(self, sample):
        # sample: T x C x H x W
        # rtype: T x 1 x H x W
        return self.video_pipeline(sample)


class AudioTransform:
    def __init__(self, subset, snr_target=None):
        if subset == "train":
            self.audio_pipeline = torch.nn.Sequential(
                AdaptiveTimeMask(6400, 16000),
                AddNoise(),
                FunctionalModule(
                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)
                ),
            )
        elif subset == "val" or subset == "test":
            self.audio_pipeline = torch.nn.Sequential(
                AddNoise(snr_target=snr_target)
                if snr_target is not None
                else FunctionalModule(lambda x: x),
                FunctionalModule(
                    lambda x: torch.nn.functional.layer_norm(x, x.shape, eps=1e-8)
                ),
            )

    def __call__(self, sample):
        # sample: T x 1
        # rtype: T x 1
        return self.audio_pipeline(sample)'''


class TextTransform:
    """Mapping Dictionary Class for SentencePiece tokenization."""

    def __init__(
        self,
        sp_model_path=SP_MODEL_PATH,
        dict_path=DICT_PATH,
    ):

        # Load SentencePiece model
        self.spm = sentencepiece.SentencePieceProcessor(model_file=sp_model_path)

        # Load units and create dictionary
        units = open(dict_path, encoding='utf8').read().splitlines()
        self.hashmap = {unit.split()[0]: unit.split()[-1] for unit in units}
        # 0 will be used for "blank" in CTC
        self.token_list = ["<blank>"] + list(self.hashmap.keys()) + ["<eos>"]
        self.ignore_id = -1

    def tokenize(self, text):
        tokens = self.spm.EncodeAsPieces(text)
        token_ids = [self.hashmap.get(token, self.hashmap["<unk>"]) for token in tokens]
        return torch.tensor(list(map(int, token_ids)))

    def post_process(self, token_ids):
        token_ids = token_ids[token_ids != -1]
        text = self._ids_to_str(token_ids, self.token_list)
        text = text.replace("\u2581", " ").strip()
        return text

    def _ids_to_str(self, token_ids, char_list):
        token_as_list = [char_list[idx] for idx in token_ids]
        return "".join(token_as_list).replace("<space>", " ")
import os

import torchaudio
import torchvision


def split_file(filename, max_frames=600, fps=25.0):

    lines = open(filename).read().splitlines()

    flag = 0
    stack = []
    res = []

    tmp = 0
    start_timestamp = 0.0

    threshold = max_frames / fps

    for line in lines:
        if "WORD START END ASDSCORE" in line:
            flag = 1
            continue
        if flag:
            word, start, end, score = line.split(" ")
            start, end, score = float(start), float(end), float(score)
            if end < tmp + threshold:
                stack.append(word)
                last_timestamp = end
            else:
                res.append(
                    [
                        " ".join(stack),
                        start_timestamp,
                        last_timestamp,
                        last_timestamp - start_timestamp,
                    ]
                )
                tmp = start
                start_timestamp = start
                stack = [word]
    if stack:
        res.append([" ".join(stack), start_timestamp, end, end - start_timestamp])
    return res


def save_vid_txt(
    dst_vid_filename, dst_txt_filename, trim_video_data, content, video_fps=25
):
    # -- save video
    save2vid(dst_vid_filename, trim_video_data, video_fps)
    # -- save text
    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)
    f = open(dst_txt_filename, "w")
    f.write(f"{content}")
    f.close()


def save_vid_aud(
    dst_vid_filename,
    dst_aud_filename,
    trim_vid_data,
    trim_aud_data,
    video_fps=25,
    audio_sample_rate=16000,
):
    # -- save video
    save2vid(dst_vid_filename, trim_vid_data, video_fps)
    # -- save audio
    save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)


def save_vid_aud_txt(
    dst_vid_filename,
    dst_aud_filename,
    dst_txt_filename,
    trim_vid_data,
    trim_aud_data,
    content,
    video_fps=25,
    audio_sample_rate=16000,
):
    # -- save video
    if dst_vid_filename is not None:
        save2vid(dst_vid_filename, trim_vid_data, video_fps)
    # -- save audio
    if dst_aud_filename is not None:
        save2aud(dst_aud_filename, trim_aud_data, audio_sample_rate)
    # -- save text
    os.makedirs(os.path.dirname(dst_txt_filename), exist_ok=True)
    f = open(dst_txt_filename, "w")
    f.write(f"{content}")
    f.close()

class LandmarksDetector:
    def __init__(self, device="cuda:0", model_name="resnet50"):
        self.face_detector = RetinaFacePredictor(
            device=device,
            threshold=0.8,
            model=RetinaFacePredictor.get_model(model_name),
        )
        self.landmark_detector = FANPredictor(device=device, model=None)

    def __call__(self, video_frames):
        landmarks = []
        for frame in video_frames:
            detected_faces = self.face_detector(frame, rgb=False)
            face_points, _ = self.landmark_detector(frame, detected_faces, rgb=True)
            if len(detected_faces) == 0:
                landmarks.append(None)
            else:
                max_id, max_size = 0, 0
                for idx, bbox in enumerate(detected_faces):
                    bbox_size = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])
                    if bbox_size > max_size:
                        max_id, max_size = idx, bbox_size
                landmarks.append(face_points[max_id])
        return landmarks
#! /usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2023 Imperial College London (Pingchuan Ma)
# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)

import os

import cv2
import numpy as np
from skimage import transform as tf


def linear_interpolate(landmarks, start_idx, stop_idx):
    start_landmarks = landmarks[start_idx]
    stop_landmarks = landmarks[stop_idx]
    delta = stop_landmarks - start_landmarks
    for idx in range(1, stop_idx - start_idx):
        landmarks[start_idx + idx] = (
            start_landmarks + idx / float(stop_idx - start_idx) * delta
        )
    return landmarks


def warp_img(src, dst, img, std_size):
    tform = tf.estimate_transform("similarity", src, dst)
    warped = tf.warp(img, inverse_map=tform.inverse, output_shape=std_size)
    warped = (warped * 255).astype("uint8")
    return warped, tform


def apply_transform(transform, img, std_size):
    warped = tf.warp(img, inverse_map=transform.inverse, output_shape=std_size)
    warped = (warped * 255).astype("uint8")
    return warped


def cut_patch(img, landmarks, height, width, threshold=5):
    center_x, center_y = np.mean(landmarks, axis=0)
    if abs(center_y - img.shape[0] / 2) > height + threshold:
        raise OverflowError("too much bias in height")
    if abs(center_x - img.shape[1] / 2) > width + threshold:
        raise OverflowError("too much bias in width")
    y_min = int(round(np.clip(center_y - height, 0, img.shape[0])))
    y_max = int(round(np.clip(center_y + height, 0, img.shape[0])))
    x_min = int(round(np.clip(center_x - width, 0, img.shape[1])))
    x_max = int(round(np.clip(center_x + width, 0, img.shape[1])))
    cutted_img = np.copy(img[y_min:y_max, x_min:x_max])
    return cutted_img


class VideoProcess:
    def __init__(
        self,
        mean_face_path="20words_mean_face.npy",
        crop_width=96,
        crop_height=96,
        start_idx=48,
        stop_idx=68,
        window_margin=12,
        convert_gray=True,
    ):
        self.reference = np.load(
            os.path.join(os.path.dirname(__file__), mean_face_path)
        )
        self.crop_width = crop_width
        self.crop_height = crop_height
        self.start_idx = start_idx
        self.stop_idx = stop_idx
        self.window_margin = window_margin
        self.convert_gray = convert_gray

    def __call__(self, video, landmarks):
        # Pre-process landmarks: interpolate frames that are not detected
        preprocessed_landmarks = self.interpolate_landmarks(landmarks)
        # Exclude corner cases: no landmark in all frames or number of frames is less than window length
        if (
            not preprocessed_landmarks
            or len(preprocessed_landmarks) < self.window_margin
        ):
            return
        # Affine transformation and crop patch
        sequence = self.crop_patch(video, preprocessed_landmarks)
        assert sequence is not None, "crop an empty patch."
        return sequence

    def crop_patch(self, video, landmarks):
        sequence = []
        for frame_idx, frame in enumerate(video):
            window_margin = min(
                self.window_margin // 2, frame_idx, len(landmarks) - 1 - frame_idx
            )
            smoothed_landmarks = np.mean(
                [
                    landmarks[x]
                    for x in range(
                        frame_idx - window_margin, frame_idx + window_margin + 1
                    )
                ],
                axis=0,
            )
            smoothed_landmarks += landmarks[frame_idx].mean(
                axis=0
            ) - smoothed_landmarks.mean(axis=0)
            transformed_frame, transformed_landmarks = self.affine_transform(
                frame, smoothed_landmarks, self.reference, grayscale=self.convert_gray
            )
            patch = cut_patch(
                transformed_frame,
                transformed_landmarks[self.start_idx : self.stop_idx],
                self.crop_height // 2,
                self.crop_width // 2,
            )
            sequence.append(patch)
        return np.array(sequence)

    def interpolate_landmarks(self, landmarks):
        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]

        if not valid_frames_idx:
            return None

        for idx in range(1, len(valid_frames_idx)):
            if valid_frames_idx[idx] - valid_frames_idx[idx - 1] > 1:
                landmarks = linear_interpolate(
                    landmarks, valid_frames_idx[idx - 1], valid_frames_idx[idx]
                )

        valid_frames_idx = [idx for idx, lm in enumerate(landmarks) if lm is not None]
        if valid_frames_idx:
            landmarks[: valid_frames_idx[0]] = [
                landmarks[valid_frames_idx[0]]
            ] * valid_frames_idx[0]
            landmarks[valid_frames_idx[-1] :] = [landmarks[valid_frames_idx[-1]]] * (
                len(landmarks) - valid_frames_idx[-1]
            )

        assert all(lm is not None for lm in landmarks), "not every frame has landmark"

        return landmarks

    def affine_transform(
        self,
        frame,
        landmarks,
        reference,
        grayscale=True,
        target_size=(256, 256),
        reference_size=(256, 256),
        stable_points=(28, 33, 36, 39, 42, 45, 48, 54),
        interpolation=cv2.INTER_LINEAR,
        border_mode=cv2.BORDER_CONSTANT,
        border_value=0,
    ):
        if grayscale:
            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        stable_reference = self.get_stable_reference(
            reference, stable_points, reference_size, target_size
        )
        transform = self.estimate_affine_transform(
            landmarks, stable_points, stable_reference
        )
        transformed_frame, transformed_landmarks = self.apply_affine_transform(
            frame,
            landmarks,
            transform,
            target_size,
            interpolation,
            border_mode,
            border_value,
        )

        return transformed_frame, transformed_landmarks

    def get_stable_reference(
        self, reference, stable_points, reference_size, target_size
    ):
        stable_reference = np.vstack([reference[x] for x in stable_points])
        stable_reference[:, 0] -= (reference_size[0] - target_size[0]) / 2.0
        stable_reference[:, 1] -= (reference_size[1] - target_size[1]) / 2.0
        return stable_reference

    def estimate_affine_transform(self, landmarks, stable_points, stable_reference):
        return cv2.estimateAffinePartial2D(
            np.vstack([landmarks[x] for x in stable_points]),
            stable_reference,
            method=cv2.LMEDS,
        )[0]

    def apply_affine_transform(
        self,
        frame,
        landmarks,
        transform,
        target_size,
        interpolation,
        border_mode,
        border_value,
    ):
        transformed_frame = cv2.warpAffine(
            frame,
            transform,
            dsize=(target_size[0], target_size[1]),
            flags=interpolation,
            borderMode=border_mode,
            borderValue=border_value,
        )
        transformed_landmarks = (
            np.matmul(landmarks, transform[:, :2].transpose())
            + transform[:, 2].transpose()
        )
        return transformed_frame, transformed_landmarks

class AVSRDataLoader:
    def __init__(self, modality, detector="retinaface", convert_gray=True):
        self.modality = modality
        if modality == "video":
            if detector == "retinaface":
                #from detectors.retinaface.detector import LandmarksDetector
                #from detectors.retinaface.video_process import VideoProcess

                self.landmarks_detector = LandmarksDetector(device="cuda:0")
                self.video_process = VideoProcess(convert_gray=convert_gray)

            if detector == "mediapipe":
                #from detectors.mediapipe.detector import LandmarksDetector
                #from detectors.mediapipe.video_process import VideoProcess

                self.landmarks_detector = LandmarksDetector()
                self.video_process = VideoProcess(convert_gray=convert_gray)

    def load_data(self, data_filename, landmarks=None, transform=True):
        if self.modality == "audio":
            audio, sample_rate = self.load_audio(data_filename)
            audio = self.audio_process(audio, sample_rate)
            return audio
        if self.modality == "video":
            video = self.load_video(data_filename)
            if not landmarks:
                landmarks = self.landmarks_detector(video)
            video = self.video_process(video, landmarks)
            if video is None:
                raise TypeError("video cannot be None")
            video = torch.tensor(video)
            return video

    def load_audio(self, data_filename):
        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)
        return waveform, sample_rate

    def load_video(self, data_filename):
        return torchvision.io.read_video(data_filename, pts_unit="sec")[0].numpy()

    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):
        if sample_rate != target_sample_rate:
            waveform = torchaudio.functional.resample(
                waveform, sample_rate, target_sample_rate
            )
        waveform = torch.mean(waveform, dim=0, keepdim=True)
        return waveform


def save2vid(filename, vid, frames_per_second):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    torchvision.io.write_video(filename, vid, frames_per_second)


def save2aud(filename, aud, sample_rate):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    torchaudio.save(filename, aud, sample_rate)

warnings.filterwarnings("ignore")

# Argument Parsing
parser = argparse.ArgumentParser(description="LRS2LRS3 Preprocessing")
parser.add_argument(
    "--data-dir",
    type=str,
    required=True,
    help="Directory of original dataset",
)
parser.add_argument(
    "--detector",
    type=str,
    default="retinaface",
    help="Type of face detector. (Default: retinaface)",
)
parser.add_argument(
    "--landmarks-dir",
    type=str,
    default=None,
    help="Directory of landmarks",
)
parser.add_argument(
    "--root-dir",
    type=str,
    required=True,
    help="Root directory of preprocessed dataset",
)
parser.add_argument(
    "--subset",
    type=str,
    required=True,
    help="Subset of dataset",
)
parser.add_argument(
    "--dataset",
    type=str,
    required=True,
    help="Name of dataset",
)
parser.add_argument(
    "--seg-duration",
    type=int,
    default=24,
    help="Max duration (second) for each segment, (Default: 24)",
)
parser.add_argument(
    "--combine-av",
    type=lambda x: (str(x).lower() == "true"),
    default=False,
    help="Merges the audio and video components to a media file.",
)
parser.add_argument(
    "--groups",
    type=int,
    default=1,
    help="Number of threads to be used in parallel.",
)
parser.add_argument(
    "--job-index",
    type=int,
    default=0,
    help="Index to identify separate jobs (useful for parallel processing).",
)
args = parser.parse_args()

seg_duration = args.seg_duration
dataset = args.dataset
text_transform = TextTransform()

# Load Data
args.data_dir = os.path.normpath(args.data_dir)
vid_dataloader = AVSRDataLoader(
    modality="video", detector=args.detector, convert_gray=False
)
aud_dataloader = AVSRDataLoader(modality="audio")

seg_vid_len = seg_duration * 25
seg_aud_len = seg_duration * 16000

# Label filename
label_filename = os.path.join(
    args.root_dir,
    "labels",
    f"{dataset}_{args.subset}_transcript_lengths_seg{seg_duration}s.csv"
    if args.groups <= 1
    else f"{dataset}_{args.subset}_transcript_lengths_seg{seg_duration}s.{args.groups}.{args.job_index}.csv",
)
os.makedirs(os.path.dirname(label_filename), exist_ok=True)
print(f"Directory {os.path.dirname(label_filename)} created")

f = open(label_filename, "w")
# Step 2, extract mouth patches from segments.
dst_vid_dir = os.path.join(
    args.root_dir, dataset, dataset + f"_video_seg{seg_duration}s"
)
dst_txt_dir = os.path.join(
    args.root_dir, dataset, dataset + f"_text_seg{seg_duration}s"
)
if dataset == "lrs3":
    if args.subset == "test":
        filenames = glob.glob(
            os.path.join(args.data_dir, args.subset, "**", "*.mp4"), recursive=True
        )
    elif args.subset == "train":
        filenames = glob.glob(
            os.path.join(args.data_dir, "trainval", "**", "*.mp4"), recursive=True
        )
        filenames.extend(
            glob.glob(
                os.path.join(args.data_dir, "pretrain", "**", "*.mp4"), recursive=True
            )
        )
        filenames.sort()
    else:
        raise NotImplementedError
elif dataset == "lrs2":
    if args.subset in ["val", "test"]:
        filenames = [
            os.path.join(args.data_dir, "main", _.split()[0] + ".mp4")
            for _ in open(
                os.path.join(os.path.dirname(args.data_dir), args.subset) + ".txt"
            )
            .read()
            .splitlines()
        ]
    elif args.subset == "train":
        filenames = [
            os.path.join(args.data_dir, "main", _.split()[0] + ".mp4")
            for _ in open(
                os.path.join(os.path.dirname(args.data_dir), args.subset) + ".txt"
            )
            .read()
            .splitlines()
        ]
        pretrain_filenames = [
            os.path.join(args.data_dir, "pretrain", _.split()[0] + ".mp4")
            for _ in open(os.path.join(os.path.dirname(args.data_dir), "pretrain.txt"))
            .read()
            .splitlines()
        ]
        filenames.extend(pretrain_filenames)
        filenames.sort()
    else:
        raise NotImplementedError

unit = math.ceil(len(filenames) * 1.0 / args.groups)
filenames = filenames[args.job_index * unit : (args.job_index + 1) * unit]

for data_filename in tqdm(filenames):
    if args.landmarks_dir:
        landmarks_filename = (
            data_filename.replace(args.data_dir, args.landmarks_dir)[:-4] + ".pkl"
        )
        landmarks = pickle.load(open(landmarks_filename, "rb"))
    else:
        landmarks = None
    try:
        video_data = vid_dataloader.load_data(data_filename, landmarks)
        audio_data = aud_dataloader.load_data(data_filename)
    except (UnboundLocalError, TypeError, OverflowError, AssertionError):
        continue

    if os.path.normpath(data_filename).split(os.sep)[-3] in [
        "trainval",
        "test",
        "main",
    ]:
        dst_vid_filename = (
            f"{data_filename.replace(args.data_dir, dst_vid_dir)[:-4]}.mp4"
        )
        dst_aud_filename = (
            f"{data_filename.replace(args.data_dir, dst_vid_dir)[:-4]}.wav"
        )
        dst_txt_filename = (
            f"{data_filename.replace(args.data_dir, dst_txt_dir)[:-4]}.txt"
        )
        trim_vid_data, trim_aud_data = video_data, audio_data
        text_line_list = (
            open(data_filename[:-4] + ".txt", "r").read().splitlines()[0].split(" ")
        )
        text_line = " ".join(text_line_list[2:])
        content = text_line.replace("}", "").replace("{", "")

        if trim_vid_data is None or trim_aud_data is None:
            continue
        video_length = len(trim_vid_data)
        audio_length = trim_aud_data.size(1)
        if video_length == 0 or audio_length == 0:
            continue
        # if audio_length/video_length < 560. or audio_length/video_length > 720. or video_length < 12:
        #    continue
        save_vid_aud_txt(
            dst_vid_filename,
            dst_aud_filename,
            dst_txt_filename,
            trim_vid_data,
            trim_aud_data,
            content,
            video_fps=25,
            audio_sample_rate=16000,
        )

        if args.combine_av:
            in1 = ffmpeg.input(dst_vid_filename)
            in2 = ffmpeg.input(dst_aud_filename)
            out = ffmpeg.output(
                in1["v"],
                in2["a"],
                dst_vid_filename[:-4] + ".av.mp4",
                vcodec="copy",
                acodec="aac",
                strict="experimental",
                loglevel="panic",
            )
            out.run()
            shutil.move(dst_vid_filename[:-4] + ".av.mp4", dst_vid_filename)

        basename = os.path.relpath(
            dst_vid_filename, start=os.path.join(args.root_dir, dataset)
        )
        token_id_str = " ".join(
            map(str, [_.item() for _ in text_transform.tokenize(content)])
        )
        f.write(
            "{}\n".format(
                f"{dataset},{basename},{trim_vid_data.shape[0]},{token_id_str}"
            )
        )
        continue

    splitted = split_file(data_filename[:-4] + ".txt", max_frames=seg_vid_len)
    for i in range(len(splitted)):
        if len(splitted) == 1:
            content, start, end, duration = splitted[i]
            trim_vid_data, trim_aud_data = video_data, audio_data
        else:
            content, start, end, duration = splitted[i]
            start_idx, end_idx = int(start * 25), int(end * 25)
            try:
                trim_vid_data, trim_aud_data = (
                    video_data[start_idx:end_idx],
                    audio_data[:, start_idx * 640 : end_idx * 640],
                )
            except TypeError:
                continue
        dst_vid_filename = (
            f"{data_filename.replace(args.data_dir, dst_vid_dir)[:-4]}_{i:02d}.mp4"
        )
        dst_aud_filename = (
            f"{data_filename.replace(args.data_dir, dst_vid_dir)[:-4]}_{i:02d}.wav"
        )
        dst_txt_filename = (
            f"{data_filename.replace(args.data_dir, dst_txt_dir)[:-4]}_{i:02d}.txt"
        )

        if trim_vid_data is None or trim_aud_data is None:
            continue
        video_length = len(trim_vid_data)
        audio_length = trim_aud_data.size(1)
        if video_length == 0 or audio_length == 0:
            continue
        save_vid_aud_txt(
            dst_vid_filename,
            dst_aud_filename,
            dst_txt_filename,
            trim_vid_data,
            trim_aud_data,
            content,
            video_fps=25,
            audio_sample_rate=16000,
        )

        if args.combine_av:
            in1 = ffmpeg.input(dst_vid_filename)
            in2 = ffmpeg.input(dst_aud_filename)
            out = ffmpeg.output(
                in1["v"],
                in2["a"],
                dst_vid_filename[:-4] + ".av.mp4",
                vcodec="copy",
                acodec="aac",
                strict="experimental",
                loglevel="panic",
            )
            out.run()
            os.remove(dst_aud_filename)
            shutil.move(dst_vid_filename[:-4] + ".av.mp4", dst_vid_filename)

         tfrecord_filename = dst_vid_filename[:-4] + ".tfrecord"
         video_frames = [cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) for frame in trim_vid_data]

         with tf.io.TFRecordWriter(tfrecord_filename) as writer:
            for frame in video_frames:
                example = create_tfrecord_example(frame)
                writer.write(example.SerializeToString())

         basename = os.path.relpath(
            dst_vid_filename, start=os.path.join(args.root_dir, dataset)
         )
         token_id_str = " ".join(
            map(str, [_.item() for _ in text_transform.tokenize(content)])
         )
         if token_id_str:
            f.write(
                "{}\n".format(
                    f"{dataset},{basename},{trim_vid_data.shape[0]},{token_id_str}"
                )
            )
f.close()

!python preprocess_lrs2lrs3.py \
    --data-dir /content/drive/MyDrive/LRS/mvlrs_v1 \
    --landmarks-dir /content/drive/MyDrive/LRS/LRS2_landmarks \
    --root-dir /content/drive/MyDrive/LRS/preprocessed \
    --dataset lrs2 \
    --subset test \
    --groups 2 \
    --job-index 1

"""**CONCATENATING ALL TRANSCRIPTS FROM ALL VIDEOS INTO A SINGLE TEXT FILE**"""

import os

def load_transcripts(directory):
    transcripts = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".txt"):
                with open(os.path.join(root, file), "r") as f:
                    transcript = f.read()
                    transcripts.append(transcript)
    return "\n".join(transcripts)

# Using the provided directory path
transcripts_directory = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/main"
text_corpus = load_transcripts(transcripts_directory)
if text_corpus != NULL:
  print("text_corpus generated successfully!")
else:
  print("Failed to generate")

import os

def load_and_save_transcripts(directory, save_path):
    transcripts = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".txt"):
                with open(os.path.join(root, file), "r") as f:
                    transcript = f.read().strip().replace(' ', ' [SPACE] ')
                    # Encapsulate each transcript with [SOS] and [EOS]
                    transcripts.append(f"[SOS] {transcript} [EOS]")
    # Join all transcripts and save to file
    concatenated_corpus = "\n".join(transcripts)
    with open(save_path, "w", encoding="utf-8") as f:
        f.write(concatenated_corpus)
    return concatenated_corpus

# Usage example
transcripts_directory = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/pretrain"
saved_corpus_path = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/pretrain_corpus.txt"
#text_corpus = load_and_save_transcripts(transcripts_directory, saved_corpus_path)

import torch
from torch.utils.data import Dataset

class CharDataset(Dataset):
    def __init__(self, filepath, block_size):
        with open(filepath, 'r', encoding='utf-8') as f:
            text = f.read()

        # Split text into chunks using newline characters as separators
        chunks = text.split('\n')

        # Define characters set
        digits = set('0123456789')
        uppercase_letters = set('ABCDEFGHIJKLMNOPQRSTUVWXYZ')
        special_tokens = {'[SPACE]', '[SOS]', '[EOS]', '[PAD]'}
        allowed_characters = digits | uppercase_letters | {"'"}

        self.vocab = sorted(allowed_characters | special_tokens)
        self.stoi = {ch: i for i, ch in enumerate(self.vocab)}
        self.itos = {i: ch for i, ch in enumerate(self.vocab)}
        self.block_size = block_size

        # Convert formatted text into indices
        self.data = []
        for chunk in chunks:
            #seq=[self.stoi['[SOS]']]
            #words=chunk.sp
            self.data.extend(self.tokenize_transcript(chunk))
        self.d=[]
        sp=0
        seq=[]
        for i in self.data:
            seq.append(i)
            #print(seq)
            if i==self.stoi['[SOS]']:
              continue
            self.d.append(seq.copy())
            if i==self.stoi['[SPACE]']:
              sp+=1
            if sp==4:
              sp-=1
              seq=seq[seq.index(self.stoi['[SPACE]'])+1:]
            if i==self.stoi['[EOS]']:
              seq=[]
              sp=0

        print(self.data[:200])
        if len(self.data) == 0:
            print("Warning: No valid characters found in text based on the current vocabulary.")

    def tokenize_transcript(self, transcript):
        tokens = [self.stoi.get(ch, self.stoi['[SPACE]']) for ch in transcript]
        tokens = [self.stoi['[SOS]']] + tokens + [self.stoi['[EOS]']]
        #print(tokens)
        return tokens

    def __len__(self):
        return max(0, len(self.data) - self.block_size)

    def __getitem__(self, idx):
        #print(len(self.d))
        sequence = self.d[idx]
        x = torch.tensor(sequence[:-1], dtype=torch.long)
        y = torch.tensor(sequence[-1:], dtype=torch.long)
        return x, y

# Create an instance of the CharDataset class
saved_corpus_path = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/saved_corpus.txt"

d = CharDataset(saved_corpus_path, block_size=128)

# Access the mapping dictionaries
char_to_index = d.stoi
index_to_char = d.itos

# Print the mappings
print("Character to Index Mapping:")
for char, index in char_to_index.items():
    print(f"{char}: {index}")

print("\nIndex to Character Mapping:")
for index, char in index_to_char.items():
    print(f"{index}: {char}")

i=0
for x,y in d:
  if i==50:
    break
  print(x,y)
  i+=1

"""**DECODER PRETRAINING**"""

#4/20/2024 new code
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import math
import os
# Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
class DecoderFFN(nn.Module):
    def __init__(self, d_model, d_ff, dropout):
        super(DecoderFFN, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.dropout(self.relu(self.linear1(x)))
        x = self.linear2(x)
        return x

class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, heads, d_ff, dropout):
        super(TransformerDecoderLayer, self).__init__()
        # Self-attention layer
        self.self_attn = nn.MultiheadAttention(d_model, heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)

        # Source-target attention layer initialized but not necessarily used
        self.cross_attn = nn.MultiheadAttention(d_model, heads, dropout=dropout,batch_first=True)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)

        # Feed-forward network
        self.ffn = DecoderFFN(d_model, d_ff, dropout)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, src, memory=None, use_cross_attention=False, src_mask=None, src_key_padding_mask=None, memory_mask=None, memory_key_padding_mask=None):
    # Self-attention on the targets (decoder's inputs)
        src2 = self.self_attn(src, src, src, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)

        print("Source tensor size after self-attention:", src.shape)

        if use_cross_attention and memory is not None:
            # Cross-attention layer where memory is the encoder's output
            print("Memory tensor size:", memory.shape)
            src2 = self.cross_attn(src, memory, memory, attn_mask=memory_mask,
                                  key_padding_mask=memory_key_padding_mask)[0]
            print(src2.shape)
            src = src + self.dropout2(src2)
            print('1',src.shape)
            src = self.norm2(src)
            print('2',src.shape)
        # Feed-forward network
        src2 = self.ffn(src)
        src = src + self.dropout3(src2)
        src = self.norm3(src)
        print(':)',src.shape)
        return src


class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout):
        super(TransformerDecoder, self).__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])
        self.norm = nn.LayerNorm(d_model)
        self.out = nn.Linear(d_model, vocab_size)
        self.d_model = d_model

    def forward(self, src, memory=None, use_cross_attention=False, src_mask=None, src_key_padding_mask=None, memory_mask=None, memory_key_padding_mask=None):
        src = self.embed(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        for layer in self.layers:
            print('hi')
            src = layer(src, memory, use_cross_attention, src_mask, src_key_padding_mask, memory_mask, memory_key_padding_mask)
        src = self.norm(src)
        output = self.out(src)
        return F.log_softmax(output, dim=-1)

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import Adam

def load_transcripts(directory):
    transcripts = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".txt"):
                with open(os.path.join(root, file), "r") as f:
                    transcript = f.read()
                    transcripts.append(transcript)
    return "\n".join(transcripts)

transcripts_directory = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/main"
text_corpus = load_transcripts(transcripts_directory)

class TextDataset(Dataset):
    def __init__(self, text, sequence_length=128):
        self.text = text
        self.sequence_length = sequence_length
        self.chars = list(set(text))
        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.chars)}
        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.chars)}
        self.data = self._prepare_data()

    def __len__(self):
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx):
        return (
            torch.tensor(self.data[idx:idx+self.sequence_length]),
            torch.tensor(self.data[idx+1:idx+self.sequence_length+1]),
        )

    def _prepare_data(self):
        return [self.char_to_idx[ch] for ch in self.text]

# Define hyperparameters
sequence_length = 128
batch_size = 64
num_epochs = 15
learning_rate = 0.001

# Initialize DataLoader
dataset = TextDataset(text_corpus, sequence_length=sequence_length)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Step 3: Train the Decoder (Character-level Language Model)
class CharLM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, bi=True):
        super(CharLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.bi=bi

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.rnn(embedded)
        output = self.fc(output)
        return output
class TextGenerate(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bi=True):
        super(TextGenerate, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        self.bi = bi

        self.encoder = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=self.bi)
        if self.bi:
          self.decoder = nn.Linear(hidden_size*2, output_size)
        else:
          self.decoder = nn.Linear(hidden_size, output_size)
        self.out = nn.Linear(output_size, output_size)
        self.dropout = nn.Dropout(0.1)

    def forward(self, input, hidden, cell):

        # encoder
        input = self.encoder(input.view(1, -1))
        input = self.dropout(input)
        output, states = self.lstm(input.view(1, 1, -1), (hidden, cell))
        output = output.permute(1, 0, 2)

        # attention
        if self.bi:
          out1, out2 = output[:,:,:self.hidden_size], output[:,:,self.hidden_size:]
          h1, h2 = states[0][states[0].size()[0] - 2,:,:], states[0][states[0].size()[0] - 1,:,:]
          attn_wts_1 = F.softmax(torch.bmm(out1, h1.unsqueeze(2)).squeeze(2), 1)
          attn_wts_2 = F.softmax(torch.bmm(out2, h2.unsqueeze(2)).squeeze(2), 1)
          attn_1 = torch.bmm(out1.transpose(1, 2), attn_wts_1.unsqueeze(2)).squeeze(2)
          attn_2 = torch.bmm(out2.transpose(1, 2), attn_wts_2.unsqueeze(2)).squeeze(2)
          attn = torch.cat((attn_1, attn_2), 1)

        else:
          h = states.squeeze(0)
          attn_wts = F.softmax(torch.bmm(output, h.unsqueeze(2)).squeeze(2), 1)
          attn = torch.bmm(output.transpose(1, 2), attn_wts.unsqueeze(2)).squeeze(2)

        # decoder
        output = self.decoder(attn)
        output = self.dropout(output)
        output = self.out(output)

        return output, states

    def init_hidden(self):
        if self.bi:
          return Variable(torch.zeros(self.n_layers*2, 1, self.hidden_size))
        else:
          return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))

    def init_cell(self):
        if self.bi:
          return Variable(torch.zeros(self.n_layers*2, 1, self.hidden_size))
        else:
          return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))
vocab_size = len(dataset.chars)
embedding_dim = 256
hidden_dim = 512
decoder_model = CharLM(vocab_size, embedding_dim, hidden_dim)

# Define loss function and optimizer
#criterion = nn.CrossEntropyLoss()
device = torch.device("cuda")
optimizer = Adam(decoder_model.parameters(), lr=learning_rate)
decoder_model.to(device)
criterion = nn.CrossEntropyLoss().to(device)

# Initialize DataLoader with pin_memory=True for faster data transfer
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)

# Training loop
for epoch in range(num_epochs):
    for batch in dataloader:
        inputs, targets = batch
        inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU
        optimizer.zero_grad()
        outputs = decoder_model(inputs)
        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))
        loss.backward()
        optimizer.step()
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

# Save the pre-trained decoder model for later use
torch.save(decoder_model.state_dict(), 'decoder_pretrained.pth')

def train_epoch(model, data_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    progress_bar = tqdm(data_loader, desc='Training')
    for x, y in progress_bar:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(x)
        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        progress_bar.set_postfix({'loss': loss.item()})
    return total_loss / len(data_loader)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
filepath = saved_corpus_path = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/saved_corpus.txt"

dataset = CharDataset(filepath, block_size=128)
vocab_size = len(dataset.vocab)
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

model = TransformerDecoder(vocab_size=vocab_size, d_model=1024, N=6, heads=16, d_ff=4096, dropout=0.1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()
saved_model_path = '/content/drive/MyDrive/model_epoch_1.pth'
if os.path.isfile(saved_model_path):
    # Load the model state dictionary
    model.load_state_dict(torch.load(saved_model_path))

    # If your filename includes the epoch number, you might extract it for setting start_epoch
    # For this example, let's manually set the start_epoch assuming you know which epoch to start from
    start_epoch = 2  # Adjust based on your saved model's epoch
else:
    start_epoch = 1
# Load and continue training if checkpoint exists
'''saved_model_path = '/content/drive/MyDrive/model_epoch_1.pth'
if os.path.isfile(saved_model_path):
    checkpoint = torch.load(saved_model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
else:
    start_epoch = 1'''

epochs = 10
for epoch in range(start_epoch, epochs + 1):
    avg_loss = train_epoch(model, data_loader, optimizer, criterion, device)
    print(f'Epoch {epoch}, Loss: {avg_loss}')
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': avg_loss,
    }, f'model_epoch_{epoch}.pth')

"""**SAMPLING**"""

sys.path.append('./')


def crop_video(opt):
    assert os.path.exists(opt.lrw_root)
    filenames = glob.glob(os.path.join(opt.lrw_root, '*', '*', '*.mp4'))
    for filename in tqdm(filenames):
        if len(filename.split('/')[-1].split('_')) == 2:
            data = utils.get_imgs_from_video(filename, RGB=True)[:, 115:211, 79:175]
            path_to_save = os.path.join(filename[:-4] + '_mouth.npz')
            np.savez(path_to_save, data=data)


class LRWClassification:
    def __init__(self, args, mode='train'):
        self.mode = mode
        self.args = args

        if mode == 'train':
            self.cur_data = glob.glob(os.path.join(args.lrw_root, '*', 'train', '*_mouth.npz'))
        elif mode == 'val':
            self.cur_data = glob.glob(os.path.join(args.lrw_root, '*', 'val', '*_mouth.npz'))
        else:
            self.cur_data = glob.glob(os.path.join(args.lrw_root, '*', 'test', '*_mouth.npz'))

        self.transform = self.get_transform()
        self.label_idx_list = list(np.load('./repo/lrw_label.npy'))

    def __getitem__(self, index):
        item = self.cur_data[index]

        video = np.load(item)['data']
        array = [Image.fromarray(f) for f in video]
        array = [self.transform(im) for im in array]
        array = torch.stack(array)

        label = item.split('/')[-3]
        label_idx = self.label_idx_list.index(label)

        return array, label_idx, label, item

    def __len__(self):
        return len(self.cur_data)

    def get_transform(self):
        if self.mode == 'train':
            return transforms.Compose([
                transforms.Grayscale(),
                transforms.RandomCrop((88, 88)),
                transforms.RandomHorizontalFlip(0.5),
                transforms.ToTensor(),
            ])
        else:
            return transforms.Compose([
                transforms.Grayscale(),
                transforms.TenCrop((88, 88)),
                transforms.Lambda(lambda crops: torch.stack(
                    [transforms.ToTensor()(crop) for crop in crops])),
            ])

import sys


startSetWordNumber = 'train/WHOLE_00313'


process_lrw(dataDir=LRW_DATA_DIR,
    saveDir=LRW_SAVE_DIR,
    startExtracting=False,
    startSetWordNumber=startSetWordNumber,
    endSetWordNumber=None,
    copyTxtFile=True,
    extractAudioFromMp4=True,
    dontWriteAudioIfExists=False,
    extractFramesFromMp4=True,
    extractOnlyWordFrames=True,
    writeFrameImages=True,
    dontWriteFrameIfExists=False,
    detectAndSaveMouths=True,
    dontWriteMouthIfExists=False,
    verbose=False)

# # DEBUG
# process_lrw(dataDir=LRW_DATA_DIR, saveDir=LRW_SAVE_DIR, startExtracting=True, startSetWordNumber='train/ACCESS_00543', endSetWordNumber=None, copyTxtFile=True, extractAudioFromMp4=True, dontWriteAudioIfExists=False, extractFramesFromMp4=True, writeFrameImages=True, dontWriteFrameIfExists=False, detectAndSaveMouths=True, dontWriteMouthIfExists=False, verbose=True)

# # TEST
# process_lrw(dataDir=LRW_DATA_DIR, saveDir='/home/voletiv/LRW-test', startExtracting=False, startSetWordNumber='val/AFTER_00050', endSetWordNumber='test/AFTERNOON_00001', copyTxtFile=False, extractAudioFromMp4=False, dontWriteAudioIfExists=False, extractFramesFromMp4=False, writeFrameImages=False, dontWriteFrameIfExists=True, detectAndSaveMouths=True, dontWriteMouthIfExists=False, verbose=True)

SHAPE_PREDICTOR_PATH = os.path.realpath('/content/drive/MyDrive/LRW/shape_predictor_68_face_landmarks.dat')

FACIAL_LANDMARKS_IDXS = dict([
    ("mouth", (48, 68)),
    ("right_eyebrow", (17, 22)),
    ("left_eyebrow", (22, 27)),
    ("right_eye", (36, 42)),
    ("left_eye", (42, 48)),
    ("nose", (27, 35)),
    ("jaw", (0, 17))
])

VIDEO_FPS = 25
VIDEO_FRAMES_PER_WORD = 30

MOUTH_SHAPE_FROM = FACIAL_LANDMARKS_IDXS["mouth"][0]
MOUTH_SHAPE_TO = FACIAL_LANDMARKS_IDXS["mouth"][1]

MOUTH_TO_FACE_RATIO = 0.65

#############################################################
# LOAD VOCAB LIST
#############################################################


def load_lrw_vocab_list(GRID_VOCAB_LIST_FILE):
    lrw_vocab = []
    with open(LRW_VOCAB_LIST_FILE) as f:
        for line in f:
            word = line.rstrip().split()[-1]
            lrw_vocab.append(word)
    return lrw_vocab

LRW_VOCAB_LIST_FILE = 'lrw_vocabulary.txt'

LRW_VOCAB = load_lrw_vocab_list(LRW_VOCAB_LIST_FILE)

"""**ROI EXTRACTION**"""

!bzip2 -d shape_predictor_68_face_landmarks.dat.bz2

!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2

!mv shape_predictor_68_face_landmarks.dat /content/drive/MyDrive/LRW/

# encoding: utf-8
import cv2
from turbojpeg import TurboJPEG, TJPF_GRAY, TJSAMP_GRAY, TJFLAG_PROGRESSIVE
import torch

import numpy as np
import glob
import time
import cv2
import os
from torch.utils.data import Dataset, DataLoader
import torch

jpeg = TurboJPEG()
def extract_opencv2(filename):
    video = []
    cap = cv2.VideoCapture(filename)
    while(cap.isOpened()):
        ret, frame = cap.read() # BGR
        if ret:
            frame = frame[115:211, 79:175]
            frame = jpeg.encode(frame)
            video.append(frame)
        else:
            break
    cap.release()
    return video

def extract_opencv(filename):
    video = []
    cap = cv2.VideoCapture(filename)

    while cap.isOpened():
        ret, frame = cap.read()  # BGR
        if ret:
            # Convert the frame to grayscale for facial landmark detection
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # Detect facial landmarks in the grayscale frame
            landmarks = predictor(gray, dlib.rectangle(0, 0, frame.shape[1], frame.shape[0]))

            # Extract the mouth region using keypoint information
            mouth_points = landmarks.parts()[48:68]  # Extract mouth keypoints (48-67)
            min_x = min(mouth_points, key=lambda x: x.x).x
            max_x = max(mouth_points, key=lambda x: x.x).x
            min_y = min(mouth_points, key=lambda x: x.y).y
            max_y = max(mouth_points, key=lambda x: x.y).y

            # Crop and encode the mouth region
            mouth_frame = frame[min_y:max_y, min_x:max_x]
            mouth_frame = jpeg.encode(mouth_frame)

            # Append the encoded mouth region to the video list
            video.append(mouth_frame)
        else:
            break

    cap.release()
    return video
target_dir = '/content/drive/MyDrive/LRW/preprocessed'

if(not os.path.exists(target_dir)):
    os.makedirs(target_dir)

class LRWDataset(Dataset):
    def __init__(self):

        with open('lrw_vocabulary.txt') as myfile:
            self.labels = myfile.read().splitlines()

        self.list = []

        for (i, label) in enumerate(self.labels[100:10]):
            files = glob.glob(os.path.join('lipread_mp4', label, '*', '*.mp4'))
            for file in files:
                savefile = file.replace('lipread_mp4', target_dir).replace('.mp4', '.pkl')
                savepath = os.path.split(savefile)[0]
                if(not os.path.exists(savepath)):
                    os.makedirs(savepath)

            files = sorted(files)


            self.list += [(file, i) for file in files]


    def __getitem__(self, idx):

        inputs = extract_opencv(self.list[idx][0])
        result = {}

        name = self.list[idx][0]
        duration = self.list[idx][0]
        labels = self.list[idx][1]


        result['video'] = inputs
        result['label'] = int(labels)
        result['duration'] = self.load_duration(duration.replace('.mp4', '.txt')).astype(np.bool)
        savename = self.list[idx][0].replace('lipread_mp4', target_dir).replace('.mp4', '.pkl')
        torch.save(result, savename)

        return result

    def __len__(self):
        return len(self.list)

    def load_duration(self, file):
        with open(file, 'r') as f:
            lines = f.readlines()
            for line in lines:
                if(line.find('Duration') != -1):
                    duration = float(line.split(' ')[1])

        tensor = np.zeros(29)
        mid = 29 / 2
        start = int(mid - duration / 2 * 25)
        end = int(mid + duration / 2 * 25)
        tensor[start:end] = 1.0
        return tensor

if(__name__ == '__main__'):
    loader = DataLoader(LRWDataset(),
            batch_size = 96,
            num_workers = 16,
            shuffle = False,
            drop_last = False)

    import time
    tic = time.time()
    for i, batch in enumerate(loader):
        toc = time.time()
        eta = ((toc - tic) / (i + 1) * (len(loader) - i)) / 3600.0
        print(f'eta:{eta:.5f}')


fig, axes = plt.subplots(1, 2)


axes[0].imshow(image1)
axes[0].axis('off')


axes[1].imshow(image2)
axes[1].axis('off')

plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/LRW/lipread_mp4/YOUNG/train
!ls -l | wc -l

"""**VISUAL MODULE**

**FRAME INTERPOLATION**
"""

#!pip install mediapy
#VIDEO INTERPOLATION
import cv2
import tensorflow as tf
import tensorflow_hub as hub
import requests
import numpy as np
import mediapy as media
from typing import Generator, Iterable, List, Optional

# Load the FILM model from TFHub
model = hub.load("https://tfhub.dev/google/film/1")

def _pad_to_align(x, align):

  assert np.ndim(x) == 4
  assert align > 0, 'align must be a positive number.'

  height, width = x.shape[-3:-1]
  height_to_pad = (align - height % align) if height % align != 0 else 0
  width_to_pad = (align - width % align) if width % align != 0 else 0

  bbox_to_pad = {
      'offset_height': height_to_pad // 2,
      'offset_width': width_to_pad // 2,
      'target_height': height + height_to_pad,
      'target_width': width + width_to_pad
  }
  padded_x = tf.image.pad_to_bounding_box(x, **bbox_to_pad)
  bbox_to_crop = {
      'offset_height': height_to_pad // 2,
      'offset_width': width_to_pad // 2,
      'target_height': height,
      'target_width': width
  }
  return padded_x, bbox_to_crop


class Interpolator:
  """A class for generating interpolated frames between two input frames.

  Uses the Film model from TFHub
  """

  def __init__(self, align: int = 64) -> None:

    self._model = hub.load("https://tfhub.dev/google/film/1")
    self._align = align

  def __call__(self, x0: np.ndarray, x1: np.ndarray,
               dt: np.ndarray) -> np.ndarray:
    """Generates an interpolated frame between given two batches of frames.

    All inputs should be np.float32 datatype.

    Args:
      x0: First image batch. Dimensions: (batch_size, height, width, channels)
      x1: Second image batch. Dimensions: (batch_size, height, width, channels)
      dt: Sub-frame time. Range [0,1]. Dimensions: (batch_size,)

    Returns:
      The result with dimensions (batch_size, height, width, channels).
    """
    if self._align is not None:
      x0, bbox_to_crop = _pad_to_align(x0, self._align)
      x1, _ = _pad_to_align(x1, self._align)

    inputs = {'x0': x0, 'x1': x1, 'time': dt[..., np.newaxis]}
    result = self._model(inputs, training=False)
    image = result['image']

    if self._align is not None:
      image = tf.image.crop_to_bounding_box(image, **bbox_to_crop)
    return image.numpy()

def _recursive_generator(
    frame1: np.ndarray, frame2: np.ndarray, num_recursions: int,
    interpolator: Interpolator) -> Generator[np.ndarray, None, None]:
  """Splits halfway to repeatedly generate more frames.

  Args:
    frame1: Input image 1.
    frame2: Input image 2.
    num_recursions: How many times to interpolate the consecutive image pairs.
    interpolator: The frame interpolator instance.

  Yields:
    The interpolated frames, including the first frame (frame1), but excluding
    the final frame2.
  """
  if num_recursions == 0:
    yield frame1
  else:
    # Adds the batch dimension to all inputs before calling the interpolator,
    # and remove it afterwards.
    time = np.full(shape=(1,), fill_value=0.5, dtype=np.float32)
    mid_frame = interpolator(
        np.expand_dims(frame1, axis=0), np.expand_dims(frame2, axis=0), time)[0]
    yield from _recursive_generator(frame1, mid_frame, num_recursions - 1,
                                    interpolator)
    yield from _recursive_generator(mid_frame, frame2, num_recursions - 1,
                                    interpolator)


def interpolate_recursively(
    frames: List[np.ndarray], num_recursions: int,
    interpolator: Interpolator) -> Iterable[np.ndarray]:
  """Generates interpolated frames by repeatedly interpolating the midpoint.

  Args:
    frames: List of input frames. Expected shape (H, W, 3). The colors should be
      in the range[0, 1] and in gamma space.
    num_recursions: Number of times to do recursive midpoint
      interpolation.
    interpolator: The frame interpolation model to use.

  Yields:
    The interpolated frames (including the inputs).
  """
  n = len(frames)
  print(num_recursions)
  for i in range(1, n):
    yield from _recursive_generator(frames[i - 1], frames[i],
                                    int(num_recursions), interpolator)
  # Separately yield the final frame.
  yield frames[-1]

import cv2
import torch
import os

def interpolate_frames(frames, target_frame_count):
    # Open the video file
    fr=frames.shape[0]
    interpolation_factor = target_frame_count / frames.shape[0]
    interpolation_factor+=1
    interpolator = Interpolator()

    # Initialize list to store interpolated frames
    interpolated_frames = []


    frames=list(frames)
    prev_frame=frames[0]
    # Read and interpolate subsequent frames
    for i in range(1,fr):
        next_frame=frames[i]
        print('test')
        print(frames[i].shape)
        print(prev_frame.unsqueeze(0).numpy().astype(np.float32).shape)
        f=list(interpolate_recursively([prev_frame.numpy().astype(np.float32),next_frame.numpy().astype(np.float32)], interpolation_factor-1,interpolator))
            #interpolated_frame = cv2.addWeighted(prev_frame, (interpolation_factor - i) / interpolation_factor, next_frame, i / interpolation_factor, 0)
            #interpolated_frames.append(interpolated_frame)
        print(':)')

        # Save the original frame
        interpolated_frames+=f
        interpolated_frames=interpolated_frames[:-1]
        # Update previous frame
        prev_frame = next_frame
    interpolated_frames.append(frames[-1])

    # Convert list of frames to tensor
    interpolated_frames = torch.tensor(interpolated_frames)


    return interpolated_frames

# Example usage

def get_max_word_length(corpus_file):
    max_length = 0
    with open(corpus_file, 'r') as file:
        for line in file:
            word = line.strip()
            if len(word) > max_length:
                max_length = len(word)
    return max_length

# Example usage
corpus_file = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/pretrain_corpus.txt"
#max_word_length = get_max_word_length(corpus_file)

#FINAL DATASET INTERPOLATED AND PADDED AS PER CONDITIONS
import os
import torch
from torch.utils.data import Dataset
from torchvision.io import read_video
from torchvision.transforms import Compose, Resize, Normalize, ToTensor
import cv2

class VideoDataset(Dataset):
    def __init__(self, root_dir, char_dataset, transform=None, cache_dir="/tmp/processed_videos", max_video_length=30):
        super(VideoDataset, self).__init__()
        self.cache_dir = cache_dir
        os.makedirs(self.cache_dir, exist_ok=True)
        self.max_video_length = max_video_length

        # Use the character-to-integer mapping from the CharDataset
        self.char_dataset=char_dataset
        #self.char_to_int = {char: i+2 for i, char in enumerate(sorted(set("ABCDEFGHIJKLMNOPQRSTUVWXYZ")))}  # Start indices from 2
        #self.char_to_int["UNK"] = 1  # Unknown characters
        #self.char_to_int["<PAD>"] = 0  # Padding token

        self.video_files = []
        self.labels = []
        i=0
        for dirname, _, filenames in os.walk(root_dir):
            for filename in filenames:

                if filename.endswith('.mp4'):
                    print(i)
                    if i==10:
                      break
                    video_file = os.path.join(dirname, filename)
                    frames, _, _ = read_video(video_file, pts_unit='sec')
                    if frames.nelement() == 0 or frames.shape[0] == 0 or frames.shape == torch.Size([0, 1, 1, 3]):
                        continue
                    if frames.nelement() == 1 or frames.shape[0] == 1 or frames.shape == torch.Size([1, 160, 160, 3]):
                        continue
                    label_file = video_file.replace('.mp4', '.txt')

                    if os.path.exists(label_file):
                        with open(label_file, 'r') as f:
                            label = f.read().strip()
                        self.video_files.append(video_file)
                        self.labels.append(label)
                        #print(label)
                    i+=1


        self.transform = transform if transform else Compose([
            Resize((224, 224)),
            ToTensor(),
            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __len__(self):
        return len(self.video_files)

    def __getitem__(self, idx):
        video_file = self.video_files[idx]
        label = self.labels[idx]

        #print(label_tensor)

        frames, _, _ = read_video(video_file, pts_unit='sec')
        label_indices = [self.char_dataset.stoi.get(char) for char in label]
        print("beforeproc:" ,frames.shape)
        print("is this working: ",len(label_indices))
        # Interpolate frames if the number of frames is less than the length of the transcript
        if frames.shape[0] < len(label_indices):
            frames = interpolate_frames(frames, len(label_indices))
            print('after inpll: ',frames.size())

        # Ensure label tensor is padded or trimmed to max_word_length
        label_indices += [0] * (frames.shape[0] - len(label_indices))
        label_tensor = torch.tensor(label_indices, dtype=torch.long)
        print('debugging:',frames.shape[0],"??",len(label_indices))
        if frames.nelement() == 0 or frames.shape[0] == 0:
            frames= torch.zeros(1,self.max_video_length, 3, 224, 224)
            return torch.zeros(self.max_video_length, 3, 224, 224),label_tensor

        if self.transform:
            # Apply transformations
            frames = frames.permute(0, 3, 1, 2)  # Convert frames from T H W C to T C H W format
            frames = self.transform(frames.float())

        print(frames.shape)
        return frames, label_tensor

"""**RESNET, LPA AND WEIGHTED DYNAMIC AGGREGATION**"""

#permutation of nmc to cnm
import torch
import torch.nn as nn
import torch.nn.functional as F

class block(nn.Module):
    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, stride=1):
        super(block, self).__init__()
        self.expansion = 4
        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(intermediate_channels)
        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(intermediate_channels)
        self.conv3 = nn.Conv2d(intermediate_channels, intermediate_channels * self.expansion, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.identity_downsample = identity_downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)

        if self.identity_downsample is not None:
            identity = self.identity_downsample(identity)

        x += identity
        x = self.relu(x)
        return x

class ResNet(nn.Module):
    def __init__(self, block, layers, image_channels, num_classes):
        super(ResNet, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNet layers
        self.layer1 = self._make_layer(block, layers[0], intermediate_channels=64, stride=1)
        self.layer2 = self._make_layer(block, layers[1], intermediate_channels=128, stride=2)
        self.layer3 = self._make_layer(block, layers[2], intermediate_channels=256, stride=2)
        self.layer4 = self._make_layer(block, layers[3], intermediate_channels=512, stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * 4, num_classes)

    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):
        identity_downsample = None
        layers = []

        if stride != 1 or self.in_channels != intermediate_channels * 4:
            identity_downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, intermediate_channels * 4, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(intermediate_channels * 4),
            )

        layers.append(block(self.in_channels, intermediate_channels, identity_downsample, stride))
        self.in_channels = intermediate_channels * 4

        for i in range(1, num_residual_blocks):
            layers.append(block(self.in_channels, intermediate_channels))  # Identity downsample is only used in the first block

        return nn.Sequential(*layers)

    def _forward_impl(self, x):
        # Helper method to process a single frame through all layers and collect outputs
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        outputs = [x]
        x = self.layer1(x)
        outputs.append(x)
        x = self.layer2(x)
        outputs.append(x)
        x = self.layer3(x)
        outputs.append(x)
        x = self.layer4(x)
        outputs.append(x)

        return outputs

    def forward(self, x):
        batch_size, time_steps, C, H, W = x.size()
        all_outputs = []

        for t in range(time_steps):
            frame = x[:, t, :, :, :]
            frame_outputs = self._forward_impl(frame)
            if not all_outputs:
                all_outputs = [[o.unsqueeze(1)] for o in frame_outputs]  # Initialize time dimension
            else:
                for i, o in enumerate(frame_outputs):
                    all_outputs[i].append(o.unsqueeze(1))  # Append along time dimension

        # Now, stack along the time dimension for each block's output
        for i in range(len(all_outputs)):
            all_outputs[i] = torch.cat(all_outputs[i], dim=1)  # Concatenate along the time dimension
            # Permute the tensor to have dimensions [batch, time_steps, channels, ni, mi]
            #all_outputs[i] = all_outputs[i].permute(0, 1, 4, 2, 3)
            print(all_outputs[i].shape)

        return all_outputs  # List of tensors with shapes [batch, time_steps, channels, ni, mi] for each scale


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class LPABlock(nn.Module):
    def __init__(self, t, ni, mi, ci, n_prime, m_prime, c_tilde, pool_kernel, pool_stride, num_heads, hidden_dim, conv_out_channels, conv_kernel_size, conv_stride):
        super(LPABlock, self).__init__()
        self.pool = nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride)
        self.conv = nn.Conv2d(ci, conv_out_channels, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_kernel_size // 2)  # Ensure padding keeps dimensions if needed
        self.fc = nn.Linear(n_prime * m_prime * conv_out_channels, n_prime * m_prime * c_tilde)
        self.position_encodings = nn.Parameter(torch.randn(1, t, n_prime * m_prime * c_tilde))
        self.ln = nn.LayerNorm(n_prime * m_prime * c_tilde)
        self.self_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads)

    def forward(self, x):
        batch, t, ni, mi, ci = x.size()
        x = x.view(batch * t, ni, mi, ci)
        x = self.pool(x)
        print('i think??')
        x = self.conv(x)
        print('am i right??')
        x = x.view(batch, t, -1)  # Flatten
        x = self.fc(x)
        x = x + self.position_encodings
        x = self.ln(x)
        x = x.permute(1, 0, 2)  # Prepare for multi-head attention
        attn_output, _ = self.self_attention(x, x, x)
        x = x + attn_output
        x = x.permute(1, 0, 2).view(batch, t, -1)
        return x





class ResNetWithLPA(ResNet):
    def __init__(self, block, layers, image_channels, num_classes, t, conv_out_channels=128, conv_kernel_size=3, conv_stride=1):
        super(ResNetWithLPA, self).__init__(block, layers, image_channels, num_classes)
        self.lpa_blocks = nn.ModuleList([
            LPABlock(t=t, ni=64, mi=56, ci=64, n_prime=28, m_prime=28, c_tilde=256, pool_kernel=(2, 2), pool_stride=(2, 2), num_heads=16, hidden_dim=1024, conv_out_channels=conv_out_channels, conv_kernel_size=conv_kernel_size, conv_stride=conv_stride),
            LPABlock(t=t, ni=128, mi=28, ci=128, n_prime=14, m_prime=14, c_tilde=512, pool_kernel=(4, 4), pool_stride=(4, 4), num_heads=16, hidden_dim=1024, conv_out_channels=conv_out_channels, conv_kernel_size=conv_kernel_size, conv_stride=conv_stride),
            LPABlock(t=t, ni=256, mi=14, ci=256, n_prime=7, m_prime=7, c_tilde=1024, pool_kernel=(2, 4), pool_stride=(2, 4), num_heads=16, hidden_dim=1024, conv_out_channels=conv_out_channels, conv_kernel_size=conv_kernel_size, conv_stride=conv_stride),
            LPABlock(t=t, ni=512, mi=7, ci=512, n_prime=4, m_prime=4, c_tilde=2048, pool_kernel=(2, 2), pool_stride=(2, 2), num_heads=16, hidden_dim=1024, conv_out_channels=conv_out_channels, conv_kernel_size=conv_kernel_size, conv_stride=conv_stride),
            LPABlock(t=t, ni=1024, mi=4, ci=1024, n_prime=2, m_prime=2, c_tilde=4096, pool_kernel=(1, 1), pool_stride=(1, 1), num_heads=16, hidden_dim=1024, conv_out_channels=conv_out_channels, conv_kernel_size=conv_kernel_size, conv_stride=conv_stride),
        ])

    def forward(self, x):
        scale_outputs = super().forward
        lpa_outputs = [lpa_block(scale_output) for lpa_block, scale_output in zip(self.lpa_blocks, scale_outputs)]
        return lpa_outputs

#model=ResNetWithLPA(block, [3, 4, 6, 3], 3, 1000, 16).to(device)

#FINAL CODE
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return x

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_model, d_k, d_v):
        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k
        self.query = nn.Linear(d_model, d_k)
        self.key = nn.Linear(d_model, d_k)
        self.value = nn.Linear(d_model, d_v)

    def forward(self, q, k, v):
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn = F.softmax(scores, dim=-1)
        output = torch.matmul(attn, v)
        return output
class LPABlock(nn.Module):
    def __init__(self, num_channels, input_height, input_width, max_pool_kernel, d_k=16, d_v=1024, d_model=1024):
        super(LPABlock, self).__init__()
        self.max_pool = nn.MaxPool2d(kernel_size=max_pool_kernel, stride=max_pool_kernel)
        self.flatten = nn.Flatten(start_dim=1)
        # Adjust the in_features based on the expected flattened shape after pooling
        pool_out_height = input_height // max_pool_kernel[0]
        pool_out_width = input_width // max_pool_kernel[1]
        self.input_proj = nn.Linear(num_channels * pool_out_height * pool_out_width, d_model)
        self.pos_enc = PositionalEncoding(d_model=d_model)
        self.layer_norm = nn.LayerNorm(d_model)
        self.attention = ScaledDotProductAttention(d_model, d_k, d_v)

    def forward(self, x):
        batch_size, time_steps, C, H, W = x.size()
        print("1: Input received")
        print(f"2: Input shape {x.size()}")
        x = x.view(-1, x.size(2), x.size(3), x.size(4))
        print(f"3: Reshaped for pooling {x.size()}")
        x = self.max_pool(x)
        print(f"4: After pooling {x.size()}")
        x = self.flatten(x)
        print(f"5: After flattening {x.size()}")

        # Ensure the input projection layer matches the flattened tensor's dimensions
        x = self.input_proj(x)
        print(f'6: After input projection {x.shape}')

        # Split batch and time dimensions again and proceed
        x = x.view(batch_size, time_steps, -1)
        print(f'7: After reshaping back {x.shape}')
        x = self.pos_enc(x)

        x = self.layer_norm(x)
        q, k, v = x, x, x  # Queries, Keys, and Values for self-attention
        x = self.attention(q, k, v)
        return x


# Define LPA blocks with specified configurations

class CombinedModel(nn.Module):
    def __init__(self, resnet, num_classes, image_channels):
        super(CombinedModel, self).__init__()
        self.resnet = resnet(block, [3, 4, 6, 3], image_channels, num_classes)  # Example configuration for ResNet50

        # Assuming the output channels of the ResNet blocks are 256, 512, 1024, and 2048 respectively
        self.lpa_blocks = nn.ModuleList([
            LPABlock(num_channels=256, max_pool_kernel=(2, 2)),  # For block1's output
            LPABlock(num_channels=512, max_pool_kernel=(4, 4)),  # For block2's output
            LPABlock(num_channels=1024, max_pool_kernel=(2, 4)),  # For block3's output
            LPABlock(num_channels=2048, max_pool_kernel=(2, 2)),  # For block4's output
            LPABlock(num_channels=2048, max_pool_kernel=(1, 1))   # For the final layer's output
        ])

    def forward(self, x):
        resnet_outputs = self.resnet(x)
        lpa_outputs = []

        for i, output in enumerate(resnet_outputs):
            lpa_output = self.lpa_blocks[i](output)
            lpa_outputs.append(lpa_output)

        return lpa_outputs

class CombinedModelWithWDA(nn.Module):
    def __init__(self, resnet, decoder_dim):
        super(CombinedModelWithWDA, self).__init__()
        self.resnet = resnet  # Use the passed-in ResNet instance

        # Assuming the output channels of the ResNet blocks are 256, 512, 1024, and 2048 respectively
        self.lpa_blocks = nn.ModuleList([
            LPABlock(num_channels=64, input_height=56, input_width=56, max_pool_kernel=(1, 1)),  # Adjusting for first output
            LPABlock(num_channels=256, input_height=56, input_width=56, max_pool_kernel=(4, 4)),  # Adjusting for second output
            LPABlock(num_channels=512, input_height=28, input_width=28, max_pool_kernel=(2, 4)),  # Adjusting for third output
            LPABlock(num_channels=1024, input_height=14, input_width=14, max_pool_kernel=(2, 2)),  # Adjusting for fourth output
            LPABlock(num_channels=2048, input_height=7, input_width=7, max_pool_kernel=(1, 1))   # Adjusting for final output
        ])

        # Weighted Dynamic Aggregation
        self.lambda_weights = nn.Parameter(torch.ones(len(self.lpa_blocks)))  # Learnable weights for each LPA block
        self.output_projection = nn.Linear(decoder_dim, decoder_dim)  # Woutput projection

        # Post-FFN
        self.ffn1 = nn.Linear(decoder_dim, decoder_dim * 4)
        self.ffn2 = nn.Linear(decoder_dim * 4, decoder_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        # The ResNet instance `self.resnet` is now used directly
        resnet_outputs = self.resnet(x)
        lpa_outputs = []

        for i, output in enumerate(resnet_outputs):
            lpa_output = self.lpa_blocks[i](output)
            lpa_outputs.append(lpa_output)

        # Weighted Dynamic Aggregation
        print(lpa_outputs)
        wda = sum(self.lambda_weights[i] * lpa_outputs[i] for i in range(len(lpa_outputs)))
        wda = self.output_projection(wda)

        # Post-FFN
        print('reached ffn')
        mc = self.ffn1(wda)
        mc = self.relu(mc)
        print('Shape of tensor before self.ffn2:', mc.shape)
        mc = self.ffn2(mc)
        print('Shape of tensor after self.ffn2:', mc.shape)
        print(mc)
        return mc


# Instantiate your model with an example configuration
# Adjust 'image_channels', 'num_classes', and 'decoder_dim' based on your specific needs
#combined_model_with_wda = CombinedModelWithWDA(ResNet, num_classes=1000, image_channels=3, decoder_dim=1024)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
from torchvision.transforms import Compose, Resize, Normalize, ToTensor

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Assuming ResNetWithLPAAndWDA is defined as per your previous messages
# Assuming VideoDataset is defined as per your previous messages
class VideoNormalize(nn.Module):
    def __init__(self, mean, std):
        super(VideoNormalize, self).__init__()
        self.mean = mean
        self.std = std

    def forward(self, tensor):
        """
        Args:
            tensor (Tensor): Video tensor to be normalized.

        Returns:
            Tensor: Normalized video tensor.
        """
        # Normalize across the channel dimension
        for t in range(tensor.size(0)):  # Loop through time dimension
            for i, (mean, std) in enumerate(zip(self.mean, self.std)):
                tensor[t, i] = (tensor[t, i] - mean) / std
        return tensor

# Transform for preprocessing the video frames
transform = Compose([
    Resize((224, 224)),
    #ToTensor(),
    VideoNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Instantiate Dataset
root_dir = '/content/drive/MyDrive/LRS/word_clips/pretrain'  # Update this path
video_dataset = VideoDataset(root_dir, char_dataset=d, transform=transform)

# DataLoader
from torch.nn.utils.rnn import pad_sequence

def custom_collate_fn(batch):
    # Separate video tensors and labels from the batch
    videos, labels = zip(*batch)

    # Pad video tensors to have the same sequence length
    # Assuming videos is a list of TxCxHxW tensors
    videos_padded = pad_sequence(videos, batch_first=True, padding_value=0)

    # Stack labels (you might need to handle labels differently depending on your setup)
    labels = torch.stack(labels, 0)

    return videos_padded, labels

import torch
from torch.nn.utils.rnn import pad_sequence

def custom_collate_fn(batch):
    videos, labels_list = zip(*batch)  # Transpose the batch (list of tuples to tuple of lists)

    # Stack videos if they are already padded to the same shape
    videos = torch.stack(videos, dim=0)

    # Convert labels into tensors and pad them
    labels = torch.stack([torch.tensor(label, dtype=torch.long) for label in labels_list], dim=0)

    return videos, labels

def custom_collates_fn(batch):
    videos, labels_list = zip(*batch)  # Transpose the batch (list of tuples to tuple of lists)

    # Stack videos if they are already padded to the same shape
    videos = torch.stack(videos, dim=0)

    # Pad labels to match the length of videos
    max_video_length = videos.size(1)  # Assuming videos have the same length
    padded_labels = pad_sequence([torch.tensor(label, dtype=torch.long) for label in labels_list], batch_first=True, padding_value=0)  # Using 0 as the padding token index

    # Trim or pad videos to ensure they have the same length
    #if padded_labels.size(1) > max_video_length:
    #    padded_labels = padded_labels[:, :max_video_length]
    if padded_labels.size(1) < max_video_length:
        padding = torch.full((padded_labels.size(0), max_video_length - padded_labels.size(1)), 0, dtype=torch.long)  # Using 0 as the padding token index
        padded_labels = torch.cat([padded_labels, padding], dim=1)

# When creating your DataLoader, specify the custom collate function
data_loader = DataLoader(video_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)

# Instantiate the Visual Module
#visual_module = ResNetWithLPAAndWDA(block, [3, 4, 6, 3], 3, 1000, t=2, d_model=1024, num_lpa_blocks=5).to(device)

# Load pre-trained decoder model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

decoder_checkpoint = torch.load('/content/drive/MyDrive/LRS/model_checkpoints/dec_model_epoch_1.pth', map_location=device)
decoder_model = TransformerDecoder(vocab_size=41, d_model=1024, N=6, heads=16, d_ff=4096, dropout=0.1).to(device)


# Load state_dict into the model
decoder_model.load_state_dict(decoder_checkpoint['model_state_dict'])

# Extract only the softmax layer
softmax_layer = decoder_model.out

# Instantiate the ResNet model with a specific configuration, for example, ResNet50
resnet_instance = ResNet(block, [3, 4, 6, 3], image_channels=3, num_classes=1000).to(device)

# Create the combined model with the ResNet instance
decoder_dim = 1024  # Example decoder dimension
visual_module = CombinedModelWithWDA(resnet_instance, decoder_dim).to(device)

# When initializing VisualModuleWithGRU, pass the d_model of ResNetWithLPAAndWDA
visual_model_with_gru = VisualModuleWithGRU(visual_module, hidden_size=1024, output_size=1024, d_model=1024, softmax_layer=softmax_layer).to(device)

"""**VISUAL MODULE OUTPUT**"""

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

# Assuming softmax_layer is extracted from the pre-trained decoder as shown earlier
#visual_model_with_gru = VisualModuleWithGRU(visual_module, hidden_size=256, output_size=vocab_size, softmax_layer=softmax_layer).to(device)


# Define your loss function and optimizer
criterion = torch.nn.CrossEntropyLoss()  # Assuming you're using CrossEntropyLoss, adjust as needed
optimizer = torch.optim.Adam(visual_model_with_gru.parameters(), lr=1e-4)

# Number of training epochs
num_epochs = 10

for epoch in range(num_epochs):
    visual_model_with_gru.train()  # Set model to training mode
    total_loss = 0.0

    # Wrap your data_loader with tqdm for a progress bar
    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{num_epochs}')

    for videos, targets in progress_bar:
        videos, targets = videos.to(device), targets.to(device)
        #print(videos[0])
        optimizer.zero_grad()
        print(targets)
        # Forward pass through your model
        outputs = visual_model_with_gru(videos)
        print("Output batch size:", outputs.size(0))
        print("Target batch size:", targets.size(0))

        # Calculate loss; you might need to adjust targets' shape to fit your needs
        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix(loss=total_loss / len(progress_bar))

    # Optionally save your model after each epoch
    torch.save({
        'epoch': epoch,
        'model_state_dict': visual_model_with_gru.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(data_loader),
    }, f'visual_module_with_gru_epoch_{epoch+1}.pth')

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}')

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import sys
import warnings

# Assuming softmax_layer is extracted from the pre-trained decoder as shown earlier
#visual_model_with_gru = VisualModuleWithGRU(visual_module, hidden_size=256, output_size=vocab_size, softmax_layer=softmax_layer).to(device)


# Define your loss function and optimizer
criterion = torch.nn.CrossEntropyLoss()  # Assuming you're using CrossEntropyLoss, adjust as needed
optimizer = torch.optim.Adam(visual_model_with_gru.parameters(), lr=1e-4)

# Number of training epochs
num_epochs = 10
null_device = open(os.devnull, 'w')
warnings.filterwarnings("ignore", category=UserWarning)

for epoch in range(num_epochs):
    visual_model_with_gru.train()  # Set model to training mode
    total_loss = 0.0

    # Wrap your data_loader with tqdm for a progress bar
    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{num_epochs}')

    for videos, targets in progress_bar:
        sys.stdout = null_device
        videos, targets = videos.to(device), targets.to(device)
        #print(videos[0])
        optimizer.zero_grad()
        print(targets)
        # Forward pass through your model
        outputs = visual_model_with_gru(videos)
        print("Output batch size:", outputs.size(0))
        print("Target batch size:", targets.size(0))

        # Calculate loss; you might need to adjust targets' shape to fit your needs
        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        sys.stdout = sys.__stdout__
        progress_bar.set_postfix(loss=total_loss / len(progress_bar))

    # Optionally save your model after each epoch
    torch.save({
        'epoch': epoch,
        'model_state_dict': visual_model_with_gru.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(data_loader),
    }, f'visual_module_with_gru_epoch_{epoch+1}.pth')

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}')

"""**CHARACTER PROBABILITY GENERATION AND GAP BRIDGING**"""

class VisualModuleWithGRU(nn.Module):
    def __init__(self, visual_module, hidden_size, output_size, d_model, num_layers=1, softmax_layer=None):
        super(VisualModuleWithGRU, self).__init__()
        self.visual_module = visual_module
        # Ensure the input size to GRU matches the d_model of the visual module
        self.gru = nn.GRU(input_size=d_model, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

        # Assign the pre-trained softmax layer
        self.softmax_layer = softmax_layer

        # Freeze the softmax layer if it's provided
        if self.softmax_layer is not None:
            for param in self.softmax_layer.parameters():
                param.requires_grad = False

    def forward(self, x):
        # Ensure x is correctly reshaped or preprocessed to match what visual_module expects
        print('Input shape:', x.shape)
        visual_output = self.visual_module(x)  # Ensure this output matches GRU's input size expectation
        print('Visual output shape:', visual_output.shape)
        gru_output, _ = self.gru(visual_output)
        print('GRU output shape:', gru_output.shape)
        fc_output = self.fc(gru_output)
        print('FC output shape:', fc_output.shape)
        print("Shape of softmax layer weight matrix:", self.softmax_layer.weight.shape)
        if self.softmax_layer is not None:
            softmax_output = self.softmax_layer(fc_output)
        else:
            softmax_output = F.log_softmax(fc_output, dim=-1)  # Fallback if no softmax layer is provided

        return softmax_output

import shutil

source_path = '/content/visual_module_with_gru_epoch_3.pth'
destination_path = '/content/drive/MyDrive/LRS/visual_module'

shutil.move(source_path, destination_path)

!mv '/content/visual_module_with_gru_epoch_1.pth' '/content/drive/MyDrive/LRS/visual_module/visual_module_with_gru_epoch_1.pth'

torch.cuda.empty_cache()

"""**SAVED MODEL**

"""

save_model(1, model, optimizer,2.78)

"""**MODEL INTEGRATION (CROSS MODEL LANGUAGE MODELING AND CROSS-MODALITY INTEGRATION)**

**PREPROCESSING DATASET FOR TRAINING**
"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchvision.io as iov

import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import json
from tqdm import tqdm
import glob
from torchvision.transforms import Compose, Resize, Normalize, ToTensor
#import ffmpeg

# Assuming ResNetWithLPAAndWDA is defined as per your previous messages
# Assuming VideoDataset is defined as per your previous messages
class VideoNormalize(nn.Module):
    def __init__(self, mean, std):
        super(VideoNormalize, self).__init__()
        self.mean = mean
        self.std = std

    def forward(self, tensor):
        """
        Args:
            tensor (Tensor): Video tensor to be normalized.

        Returns:
            Tensor: Normalized video tensor.
        """
        # Normalize across the channel dimension
        for t in range(tensor.size(0)):  # Loop through time dimension
            for i, (mean, std) in enumerate(zip(self.mean, self.std)):
                tensor[t, i] = (tensor[t, i] - mean) / std
        return tensor

class PreprocessedDataset(Dataset):
    def __init__(self, root_dir, dataset, subset, stoi):
        self.root_dir = root_dir
        self.dataset = dataset
        self.subset = subset
        self.stoi = stoi  # Dictionary mapping characters to indices
        self.labels_dir = os.path.join(root_dir, 'labels')
        self.video_dir = os.path.join(root_dir, dataset, f'{dataset}_video_seg24s/')
        self.text_dir = os.path.join(root_dir, dataset, f'{dataset}_text_seg24s/')

        self.data = self._load_data()
        #self.max_frames = self._get_max_frames()  # Store the maximum number of frames in a video
        self.transform = transforms.Compose([
            transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),
            transforms.Resize((224, 224)),  # Adjust the dimension to NCHW
            VideoNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def _get_max_frames(self):
        max_frames = 0
        for video_path, _ in self.data:
            try:
                info = ffmpeg.probe(video_path)
                num_frames = int(info['streams'][0]['nb_frames'])
                max_frames = max(max_frames, num_frames)
            except ffmpeg.Error as e:
                print(f"Warning: Unable to get frame count for {video_path}: {e.stderr}")
        return max_frames

    def _video_to_transcript_path(self, video_path):
        relative_video_path = video_path.replace(self.video_dir, "")
        transcript_path = os.path.join(self.text_dir, relative_video_path).replace(".mp4", ".txt")
        return transcript_path

    def _load_data(self):
        data = []
        label_files = glob.glob(os.path.join(self.labels_dir, f'{self.dataset}_{self.subset}_transcript_lengths_seg24s*.csv'))
        #print(len(label_files))
        label_files=label_files[0:3]
        #print(label_files)
        for label_file in label_files:
            with open(label_file, 'r') as f:
                lines = f.readlines()
                for line in lines:
                    parts = line.strip().split(',')
                    video_name = parts[1].strip()
                    video_path = os.path.join(self.root_dir, self.dataset, video_name)
                    transcript_path = self._video_to_transcript_path(video_path)
                    if os.path.exists(video_path):
                        data.append((video_path, transcript_path))
                    else:
                        print(f"Warning: Video file not found at {video_path}")
        return data

    def tokenize_transcript(self, transcript):
        tokens = [self.stoi['[SOS]']] + [self.stoi[ch] if ch in self.stoi else self.stoi['[SPACE]'] for ch in transcript] + [self.stoi['[EOS]']]
        print(tokens)
        return torch.tensor(tokens, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        video_path, transcript_path = self.data[idx]

        video_frames, _, _ = iov.read_video(video_path, pts_unit='sec')
        print(f"Video frames shape: {video_frames.shape}")
        video_frames = video_frames.float() / 255.0
        video_frames = self.transform(video_frames.float())  # Apply transformations
        print(f"Video frames shape: {video_frames.shape}")
        with open(transcript_path, 'r') as f:
            transcript = f.read().strip()
            print(transcript)
            tokenized_transcript = self.tokenize_transcript(transcript)
            print(tokenized_transcript)

        return video_frames, tokenized_transcript

root_dir = '/content/drive/MyDrive/LRS/preprocessed'
#dataset = 'lrs2'
#subset = 'train'

preprocessed_dataset = PreprocessedDataset(root_dir, 'lrs2', 'train', stoi=d.stoi)
dataloader = DataLoader(preprocessed_dataset, batch_size=1, shuffle=True)

    # videos: CUDA tensor of shape (batch_size, num_frames, height, width, channels)
    # transcripts: List of transcripts for each video in the batch

!pip install ffmpeg-python #for processing the videos

i=0
for x,y in dataloader:
  print(x.size())
  if i==5:
    break
  print(y.size())
  i+=1

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# Assuming the necessary imports for CombinedModelWithWDA, TransformerDecoder, ResNet, LPABlock, PreprocessedDataset

def load_visual_module_from_combined(pretrained_path, combined_model):
    saved_state_dict = torch.load(pretrained_path)['model_state_dict']
    visual_module_state_dict = {}
    prefix = 'visual_module.'  # Prefix to identify visual module parameters

    for key, value in saved_state_dict.items():
        if key.startswith(prefix) and 'gru' not in key:  # Exclude GRU parameters
            new_key = key[len(prefix):]
            visual_module_state_dict[new_key] = value

    combined_model.visual_module.load_state_dict(visual_module_state_dict, strict=False)
    print("Visual module loaded successfully without GRU.")

def init_target_tensor(batch_size, sos_token_id, device):
    # Assuming `sos_token_id` is the token ID for the start-of-sequence token
    return torch.full((1, batch_size), sos_token_id, dtype=torch.long, device=device)

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    vocab_size = len(d.vocab)  # This needs to be defined correctly before use
    decoder_instance = TransformerDecoder(vocab_size=vocab_size, d_model=1024, N=6, heads=16, d_ff=4096, dropout=0.1).to(device)

    decoder_checkpoint = torch.load('/content/drive/MyDrive/LRS/dec_model_epoch_1.pth', map_location=device)
    decoder_instance.load_state_dict(decoder_checkpoint['model_state_dict'])

    softmax_layer=decoder_instance.out

resnet_instance = ResNet(block, [3, 4, 6, 3], image_channels=3, num_classes=1000).to(device)
decoder_dim=1024
visual_module_instance = CombinedModelWithWDA(resnet_instance, decoder_dim).to(device)  # Assuming instantiation details are handled


combined_model = VisualModuleWithGRU(visual_module_instance, hidden_size=1024, output_size=1024, d_model=1024, softmax_layer=softmax_layer).to(device)

load_visual_module_from_combined('/content/drive/MyDrive/LRS/visual_module/visual_module_with_gru_epoch_3.pth', combined_model)

visual_module=combined_model.visual_module

# Training setup
optimizer = optim.Adam(list(visual_module.parameters()) + list(decoder_instance.parameters()), lr=0.001)
criterion = nn.CTCLoss(blank=0, zero_infinity=True).to(device)  # Or any other suitable loss function

# Dataset and DataLoader setup
#dataset = PreprocessedDataset('path_to_dataset', 'lrs2', 'train')
#dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
sos_token_id = 39
# Training loop

optimizer = optim.Adam(list(visual_module.parameters()) + list(decoder_instance.parameters()), lr=0.001)
criterion = nn.CTCLoss(blank=0, zero_infinity=True).to(device)  # Or any other suitable loss function

"""**TRAINING THE MODEL OBTAINED AFTER CROSS-MODALITY INTEGRATION**"""

import os
import sys
null_device = open(os.devnull, 'w')

epochs = 10
for epoch in range(epochs):
    visual_module.train()
    decoder_instance.train()
    total_loss = 0.0
    #sys.stdout = null_device
    for video_frames, transcripts in tqdm(dataloader):
        #sys.stdout = sys.__stdout__
        print(video_frames.size(),transcripts.size())
        #if video_frames.size(1)<transcripts.size(1):
            #continue
        #print(video_frames,transcripts)
        #transcripts = tokenize_transcript(transcripts[0], dataset.stoi)  # Assuming dataset.stoi is accessible
        #transcripts = torch.tensor(transcripts, dtype=torch.long).to(device).unsqueeze(0)  # Add batch dimension if needed
        #sys.stdout = null_device
        torch.autograd.set_detect_anomaly(True)
        video_frames, transcripts = video_frames.to(device), transcripts.to(device)
        #print(transcripts)
        torch.cuda.empty_cache()
        # Initialize the input for the auto-regressive generation
        #target_input = init_target_tensor(transcripts.size(0), sos_token_id, device)

        optimizer.zero_grad()

        # Get the multi-motion-informed context from the visual module
        MC = visual_module(video_frames)
        if torch.isnan(MC).any():
            print("NaN detected in MC, skipping batch")
            continue  # Skip this batch

        print('mc',MC)
        print(MC.size())
        MC = MC.transpose(0, 1)

        # Sequentially predict each character
        target_input = init_target_tensor(transcripts.size(0), sos_token_id, device)

        predictions = []
        for i in range(video_frames.size(1)+20):  # Assuming transcripts have been properly padded
            if i == 0:
                # Start the sequence with [SOS] token
                output = decoder_instance(target_input, MC, use_cross_attention=True)
            else:
                last_token_indices = output[:, -1, :].max(dim=-1)[1].unsqueeze(1).type(torch.long)

                # Continue the sequence with the last predicted token
                output = decoder_instance(last_token_indices, MC, use_cross_attention=True)

            predictions.append(output)

        # Concatenate predictions along the sequence dimension
        predictions = torch.cat(predictions, dim=1)
        predictions = predictions.transpose(0, 1)  # Change it to (T, N, C)
        input_lengths = torch.tensor([predictions.size(1)], dtype=torch.long, device=device)  # Length of the sequence, not batch
        target_lengths = torch.tensor([transcripts.size(1)], dtype=torch.long, device=device)  # Length of each target transcript
# Calculate loss
        loss = criterion(predictions,  # Log softmax along vocabulary dimension
                         transcripts,
                         input_lengths,
                         target_lengths)
        if torch.isnan(loss):
            print("NaN detected in loss, skipping batch")
            continue  # Skip this batch on NaN loss

        loss.backward()  # Compute gradients
        torch.nn.utils.clip_grad_norm_(decoder_instance.parameters(), max_norm=1)

        optimizer.step()  # Update parameters
        #sys.stdout = sys.__stdout__
        total_loss += loss.item()


    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')

    # Save the model
    torch.save({
        'epoch': epoch,
        'visual_module_state_dict': visual_module.state_dict(),
        'decoder_state_dict': decoder_instance.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
    }, f'/content/drive/MyDrive/LRS/finalmodel_epoch_{epoch+1}.pth')

#new
import os
import sys
null_device = open(os.devnull, 'w')

epochs = 10
for epoch in range(epochs):
    visual_module.train()
    decoder_instance.train()
    total_loss = 0.0
    #sys.stdout = null_device
    for video_frames, transcripts in tqdm(dataloader):
        #sys.stdout = sys.__stdout__
        print(video_frames.size(),transcripts.size())
        #if video_frames.size(1)<transcripts.size(1):
            #continue
        #print(video_frames,transcripts)
        #transcripts = tokenize_transcript(transcripts[0], dataset.stoi)  # Assuming dataset.stoi is accessible
        #transcripts = torch.tensor(transcripts, dtype=torch.long).to(device).unsqueeze(0)  # Add batch dimension if needed
        #sys.stdout = null_device
        torch.autograd.set_detect_anomaly(True)
        video_frames, transcripts = video_frames.to(device), transcripts.to(device)
        #print(transcripts)
        torch.cuda.empty_cache()
        # Initialize the input for the auto-regressive generation
        #target_input = init_target_tensor(transcripts.size(0), sos_token_id, device)

        optimizer.zero_grad()

        # Get the multi-motion-informed context from the visual module
        MC = visual_module(video_frames)
        if torch.isnan(MC).any():
            print("NaN detected in MC, skipping batch")
            continue  # Skip this batch

        print('mc',MC)
        print(MC.size())
        #MC = MC.transpose(0, 1)

        # Sequentially predict each character
        target_input = torch.tensor([[39]], dtype=torch.long, device=device)  # Add batch dimension

        predictions = []
        sp = 0
        for i in range(transcripts.size(1)):  # Assuming transcripts have been properly padded
            output = decoder_instance(target_input, MC, use_cross_attention=True)
            last_token_indices = output[:, -1, :].max(dim=-1)[1].unsqueeze(1).type(torch.long)
            max_prob_index = output[:, -1, :].argmax(dim=-1).item()
            print('ind: ', max_prob_index)

            # Append the index to target_input
            target_input = torch.cat([target_input, torch.tensor([[max_prob_index]], dtype=torch.long, device=device)], dim=1)  # Add new index along sequence dimension

            if max_prob_index == d.stoi['[SPACE]']:
                sp += 1
            predictions.append(output)

            # Check if the predicted token is EOS
            if max_prob_index == d.stoi['[EOS]']:
                break

        # Concatenate predictions along the sequence dimension
        predictions = torch.cat(predictions, dim=1)
        predictions = predictions.transpose(0, 1)  # Change it to (T, N, C)
        input_lengths = torch.tensor([predictions.size(1)], dtype=torch.long, device=device)  # Length of the sequence, not batch
        target_lengths = torch.tensor([transcripts.size(1)], dtype=torch.long, device=device)  # Length of each target transcript
# Calculate loss
        loss = criterion(predictions,  # Log softmax along vocabulary dimension
                         transcripts,
                         input_lengths,
                         target_lengths)
        if torch.isnan(loss):
            print("NaN detected in loss, skipping batch")
            continue  # Skip this batch on NaN loss

        loss.backward()  # Compute gradients
        torch.nn.utils.clip_grad_norm_(decoder_instance.parameters(), max_norm=1)

        optimizer.step()  # Update parameters
        #sys.stdout = sys.__stdout__
        total_loss += loss.item()


    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')

    # Save the model
    torch.save({
        'epoch': epoch,
        'visual_module_state_dict': visual_module.state_dict(),
        'decoder_state_dict': decoder_instance.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
    }, f'/content/drive/MyDrive/LRS/finalmodel_epoch_{epoch+1}.pth')

import os
import sys
from tqdm import tqdm

null_device = open(os.devnull, 'w')

epochs = 10
for epoch in range(epochs):
    torch.cuda.empty_cache()
    visual_module.train()
    decoder_instance.train()
    total_loss = 0.0
    sys.stdout = null_device
    for video_frames, transcripts in tqdm(dataloader):
        torch.autograd.set_detect_anomaly(True)
        video_frames, transcripts = video_frames.to(device), transcripts.to(device)
        torch.cuda.empty_cache()
        optimizer.zero_grad()
        MC = visual_module(video_frames)
        if torch.isnan(MC).any():
            print("NaN detected in MC, skipping batch")
            continue

        target_input = torch.tensor([[39]], dtype=torch.long, device=device)  # Add batch dimension
        predictions = []
        sp = 0
        for i in range(transcripts.size(1)):  # Assuming transcripts have been properly padded
            output = decoder_instance(target_input, MC, use_cross_attention=True)
            last_token_indices = output[:, -1, :].max(dim=-1)[1].unsqueeze(1).type(torch.long)
            max_prob_index = output[:, -1, :].argmax(dim=-1).item()

            # Append the index to target_input
            target_input = torch.cat([target_input, torch.tensor([[max_prob_index]], dtype=torch.long, device=device)], dim=1)

            if max_prob_index == d.stoi['[SPACE]']:
                sp += 1
            if sp==4:
                sp-=1

            predictions.append(output)

            # Check if the predicted token is EOS
            if max_prob_index == d.stoi['[EOS]']:
                break

        # Concatenate predictions along the sequence dimension
        predictions = torch.cat(predictions, dim=1)
        predictions = predictions.transpose(0, 1)  # Change it to (T, N, C)
        input_lengths = torch.tensor([predictions.size(1)], dtype=torch.long, device=device)  # Length of the sequence, not batch
        target_lengths = torch.tensor([transcripts.size(1)], dtype=torch.long, device=device)  # Length of each target transcript
        sys.stdout = sys.__stdout__
        # Calculate loss
        loss = criterion(predictions, transcripts, input_lengths, target_lengths)
        if torch.isnan(loss):
            print("NaN detected in loss, skipping batch")
            continue  # Skip this batch on NaN loss

        loss.backward()  # Compute gradients
        torch.nn.utils.clip_grad_norm_(decoder_instance.parameters(), max_norm=1)
        optimizer.step()  # Update parameters
        total_loss += loss.item()

    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')

    # Save the model
    torch.save({
        'epoch': epoch,
        'visual_module_state_dict': visual_module.state_dict(),
        'decoder_state_dict': decoder_instance.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
    }, f'/content/drive/MyDrive/LRS/finalmodel_epoch_{epoch+1}.pth')

"""**SAVING MODEL OBTAINED AFTER CROSS-MODALITY INTEGRATION**"""

torch.save({
        'epoch': epoch,
        'visual_module_state_dict': visual_module.state_dict(),
        'decoder_state_dict': decoder_instance.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
}, f'/content/drive/MyDrive/LRS/finalmodel_epoch_{epoch+1}.pth')

torch.save({
        'epoch': 1,
        'visual_module_state_dict': visual_module.state_dict(),
        'decoder_state_dict': decoder_instance.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': total_loss / len(dataloader),
    }, f'/content/drive/MyDrive/LRS/finalmodel_epoch_{epoch+1}.pth')

"""**INFERENCE MODULE**"""

import os
import torch
from torch.utils.data import Dataset
from torchvision.io import read_video
from torchvision.transforms import Compose, Resize, Normalize, ToTensor
import cv2
import tensorflow as tf
import numpy as nm
import imageio

class SA(Dataset):
    def __init__(self, input_base_path, char_dataset, t_base, transform=None, cache_dir="/tmp/processed_videos", max_video_length=30):
        super(VideoDataset, self).__init__()
        self.cache_dir = cache_dir
        os.makedirs(self.cache_dir, exist_ok=True)
        self.max_video_length = max_video_length
        self.t_base=t_base
        self.char_dataset=char_dataset

        self.video_files = []
        self.labels = []
        i=0
        for root, dirs, files in os.walk(input_base_path):
            print('root: ',root)
            print('dir: ',dirs)
            print('files: ',files)
            if i>10:
              break
            for file in files:
                print(file)
                i+=1
                if i==10:
                  break
                if file.endswith(".mp4"):
                    i+=1
                    print(dir)
                    base_filename = file.replace('.mp4', '')
                    label_name = f"{base_filename}.txt"
                    video_path = os.path.join(root, file)
                    dr=root.split('/')[-1]
                    print('vid: ',video_path)
                    transcript_path = os.path.join(self.t_base,dr, label_name)
                    print('t: ',transcript_path)
                    if os.path.exists(transcript_path):
                        with open(transcript_path, 'r') as f:
                            label = f.read().strip()
                        self.labels.append(label)
                        print()
                        self.video_files.append(video_path)

                    # Determine output paths
                    '''relative_path = os.path.relpath(root, input_base_path)
                    output_dir = os.path.join(output_base_path, relative_path)
                    os.makedirs(output_dir, exist_ok=True)

                    if not os.path.exists(video_path):
                        print(f"Video file {video_path} does not exist.")
                        continue

                    # Segment the video
                    segment_video(video_path, transcript_path, output_dir, base_filename)

                    print(f"Finished processing segments for {video_name}")'''

    def __len__(self):
        return len(self.video_files)

    def __getitem__(self, idx):
        video_file, label = self.video_files[idx], self.labels[idx]
        print(video_file)
        # Read video using imageio
        reader = imageio.get_reader(video_file)
        frames = []
        for frame in reader:
            # Resize frame to (112, 112)
            #print(frame.shape)
            frame = cv2.resize(frame, (112, 112))
            #print(frame.shape)
            # Convert frame to grayscale
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            #print(frame.shape)
            frames.append(frame)
        reader.close()

        # Check if frames are captured
        if not frames:
            return None, label

        # Stack frames into a tensor of shape (num_frames, height, width, channels)
        frames = np.array(frames)
        #print('hi')
        #print(frames.shape)
        frames = np.mean(frames, axis=0)
        #print(frames.shape)
        frames = frames / 255
        frames = np.expand_dims(frames, axis=0)
        #print(frames.shape)
        return tf.convert_to_tensor(frames), label

input_base_path = '/content/drive/MyDrive/LRS/mvlrs_v1/main'
t_base = "/content/drive/MyDrive/LRS/preprocessed/lrs2/lrs2_text_seg24s/main"
ds = SA(input_base_path,d.stoi,t_base)

!pip install keras_cv

import tensorflow as tf
from tensorflow import keras
from keras.models import load_model
import keras_cv
import numpy as np

s_analysis = keras.models.load_model('/content/drive/MyDrive/sent_analysis.h5')
s_analysis.trainable = False

for j,i in ds:

  #print(i)
  print(np.argmax(s_analysis.predict(j)))

vocab_size = len(d.vocab)  # This needs to be defined correctly before use
    #print(vocab_size)
    # Initialize the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
decoder_instance = TransformerDecoder(vocab_size=vocab_size, d_model=1024, N=6, heads=16, d_ff=4096, dropout=0.1).to(device)

resnet_instance = ResNet(block, [3, 4, 6, 3], image_channels=3, num_classes=1000).to(device)
decoder_dim=1024
visual_module_instance = CombinedModelWithWDA(resnet_instance, decoder_dim).to(device)  # Assuming instantiation details are handled

"""**EXTERNAL SENTIMENT ANALYSIS MODEL**"""

!pip install transformers

from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification, pipeline

tokenizer = RobertaTokenizerFast.from_pretrained("arpanghoshal/EmoRoBERTa")
s_model = TFRobertaForSequenceClassification.from_pretrained("arpanghoshal/EmoRoBERTa")

emotion = pipeline('sentiment-analysis',
                    model='arpanghoshal/EmoRoBERTa')

emotion_labels = emotion("This is so scary")
print(emotion_labels)

class CombinedModel(nn.Module):
    def __init__(self, visual_module, decoder):
        super(CombinedModel, self).__init__()
        self.visual_module = visual_module
        self.decoder = decoder
        self.sos_token_id=39
        self.s_analysis = keras.models.load_model('/content/drive/MyDrive/sent_analysis.h5')
        self.s_analysis.trainable = False
    def forward(self, x, state=None, step=False):
            video_frames=x
            # Full sequence processing for training
            memory = self.visual_module(x)
            #print(memory.size())
            #memory = memory.transpose(0,1)  # Ensure memory is in shape [batch_size, seq_len, features]
            batch_size = 1
            initial_input = torch.full((1, batch_size), self.sos_token_id, dtype=torch.long, device=memory.device)

            predictions=[]
            for i in range(video_frames.size(1)+20):  # Assuming transcripts have been properly padded
                if i == 0:
                    # Start the sequence with [SOS] token
                    output = decoder_instance(initial_input, memory, use_cross_attention=True)
                else:
                    last_token_indices = output[:, -1, :].max(dim=-1)[1].unsqueeze(1).type(torch.long)
                    output = decoder_instance(last_token_indices, memory, use_cross_attention=True)

                predictions.append(output)
            return predictions

"""**LOAD TEST CASES**"""

import os
import torch
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchvision.io as iov

import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import json
from tqdm import tqdm
import glob
from torchvision.transforms import Compose, Resize, Normalize, ToTensor
#import ffmpeg

# Assuming ResNetWithLPAAndWDA is defined as per your previous messages
# Assuming VideoDataset is defined as per your previous messages
class VideoNormalize(nn.Module):
    def __init__(self, mean, std):
        super(VideoNormalize, self).__init__()
        self.mean = mean
        self.std = std

    def forward(self, tensor):
        """
        Args:
            tensor (Tensor): Video tensor to be normalized.

        Returns:
            Tensor: Normalized video tensor.
        """
        # Normalize across the channel dimension
        for t in range(tensor.size(0)):  # Loop through time dimension
            for i, (mean, std) in enumerate(zip(self.mean, self.std)):
                tensor[t, i] = (tensor[t, i] - mean) / std
        return tensor

class PreprocessedDataset(Dataset):
    def __init__(self, root_dir, dataset, subset, stoi):
        self.root_dir = root_dir
        self.dataset = dataset
        self.subset = subset
        self.stoi = stoi  # Dictionary mapping characters to indices
        self.labels_dir = os.path.join(root_dir, 'labels')
        self.video_dir = os.path.join(root_dir, dataset, f'{dataset}_video_seg24s/')
        self.text_dir = os.path.join(root_dir, dataset, f'{dataset}_text_seg24s/')

        self.data = self._load_data()
        #self.max_frames = self._get_max_frames()  # Store the maximum number of frames in a video
        self.transform = transforms.Compose([
            transforms.Lambda(lambda x: x.permute(0, 3, 1, 2)),
            transforms.Resize((224, 224)),  # Adjust the dimension to NCHW
            VideoNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def _get_max_frames(self):
        max_frames = 0
        for video_path, _ in self.data:
            try:
                info = ffmpeg.probe(video_path)
                num_frames = int(info['streams'][0]['nb_frames'])
                max_frames = max(max_frames, num_frames)
            except ffmpeg.Error as e:
                print(f"Warning: Unable to get frame count for {video_path}: {e.stderr}")
        return max_frames

    def _video_to_transcript_path(self, video_path):
        relative_video_path = video_path.replace(self.video_dir, "")
        transcript_path = os.path.join(self.text_dir, relative_video_path).replace(".mp4", ".txt")
        return transcript_path

    def _load_data(self):
        data = []
        label_files = glob.glob(os.path.join(self.labels_dir, f'{self.dataset}_{self.subset}_transcript_lengths_seg24s*.csv'))
        #print(len(label_files))
        label_files=label_files[0:2]
        #print(label_files)
        for label_file in label_files:
            with open(label_file, 'r') as f:
                lines = f.readlines()
                for line in lines:
                    parts = line.strip().split(',')
                    video_name = parts[1].strip()
                    video_path = os.path.join(self.root_dir, self.dataset, video_name)
                    aud=os.path.join(self.root_dir, self.dataset, video_name).replace(".mp4", ".wav")
                    transcript_path = self._video_to_transcript_path(video_path)
                    if os.path.exists(video_path):
                        data.append((video_path, transcript_path, aud))
                    else:
                        print(f"Warning: Video file not found at {video_path}")
        return data

    def tokenize_transcript(self, transcript):
        tokens = [self.stoi['[SOS]']] + [self.stoi[ch] if ch in self.stoi else self.stoi['[SPACE]'] for ch in transcript] + [self.stoi['[EOS]']]
        print(tokens)
        return torch.tensor(tokens, dtype=torch.long)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        video_path, transcript_path, a = self.data[idx]

        video_frames, _, _ = iov.read_video(video_path, pts_unit='sec')
        #print(video_frames.shape[0])
        #video_frames=interpolate_frames(video_frames, video_frames.shape[0]*2)
        print(f"Video frames shape: {video_frames.shape}")
        #video_frames = video_frames.permute(0, 3, 1, 2)  # Convert frames from T H W C to T C H W format
        #video_frames = video_frames.float() / 255.0  # Normalize pixel values to [0, 1]
        video_frames = video_frames.float() / 255.0
        video_frames = self.transform(video_frames.float())  # Apply transformations
        #video_frames=torch.unsqueeze(video_frames,0)
        try:
          with sr.AudioFile(a) as source:
              audio = recognizer.record(source)
              a=recognizer.recognize_google(audio)
        except:
          a=''
        print(f"Video frames shape: {video_frames.shape}")
        with open(transcript_path, 'r') as f:
            transcript = f.read().strip()
            print(transcript)
            tokenized_transcript = self.tokenize_transcript(transcript)
            print(tokenized_transcript)
        #print(a)


        return video_frames, transcript, a

root_dir = '/content/drive/MyDrive/LRS/preprocessed'
#dataset = 'lrs2'
#subset = 'train'

test_dset = PreprocessedDataset(root_dir, 'lrs2', 'test', stoi=d.stoi)
dataloader = DataLoader(test_dset, batch_size=1, shuffle=True)

    # videos: CUDA tensor of shape (batch_size, num_frames, height, width, channels)
    # transcripts: List of transcripts for each video in the batch

"""**LOADS COMBINED MODEL**"""

import tensorflow as tf
from tensorflow import keras
from keras.models import load_model
import keras_cv
def load_model(model_path, visual_module, decoder):
    checkpoint = torch.load(model_path)
    visual_module.load_state_dict(checkpoint['visual_module_state_dict'])
    decoder.load_state_dict(checkpoint['decoder_state_dict'])

    return CombinedModel(visual_module, decoder)

# Assuming visual_module_instance and decoder_instance are properly initialized instances of your modules
model_path = '/content/drive/MyDrive/LRS/finalmodel_epoch_1.pth'
combined_model = load_model(model_path, visual_module_instance, decoder_instance)
combined_model = combined_model.to(device)
combined_model.eval()  # Set the model to evaluation mode

"""**BEAM SEARCH**"""

import torch

def beam_search_decoder(probabilities, beam_width=12):
    """
    Perform beam search decoding given a sequence of softmax probabilities from a model.

    Arguments:
    probabilities -- Tensor of shape (sequence_length, vocab_size) containing softmax probabilities.
    beam_width -- int, the number of sequences to consider at each step (beam width).

    Returns:
    top_sequences -- List of tuples (sequence, probability) for the top beam_width sequences.
    """
    # Initialize the beam with the start of the sequence
    # Each element in the beam is a tuple (probability, [sequence of token indices])
    start_token_idx = 39  # Assuming 0 is the index of the start token [SOS]
    beam = [(1.0, [start_token_idx])]
    print(probabilities)
    # Iterate over the sequence of probabilities
    for step in range(1, probabilities.size(0)):
        new_beam = []
        for prob, seq in beam:
            # Consider all possible next tokens
            for token_idx in range(probabilities.size(1)):
                new_prob = prob * probabilities[step, token_idx].item()
                new_seq = seq + [token_idx]
                new_beam.append((new_prob, new_seq))

        # Sort all sequences in the new_beam by probability in descending order
        new_beam.sort(reverse=True, key=lambda x: x[0])

        # Select the top beam_width sequences to keep in the beam
        beam = new_beam[:beam_width]

    return beam

# Example usage:
# Assuming `softmax_outputs` is a tensor from your model of shape (sequence_length, vocab_size)
# You would call this function like this:
# beam_results = beam_search_decoder(softmax_outputs, beam_width=3)

# Each entry in beam_results is a tuple (probability, sequence of indices)

def perform_inference(model, video_frames, beam_width=12):
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():  # Disable gradient computation
        softmax_outputs = model(video_frames)  # Assume this returns softmax probabilities of shape [seq_len, batch_size, vocab_size]
    print('output: ',softmax_outputs)
    # Assuming softmax_outputs are returned with batch first
    softmax_outputs = softmax_outputs[0]  # Remove the batch dimension if batch_size is 1
    beam_results = beam_search_decoder(softmax_outputs, beam_width=beam_width)
    return beam_results

"""**PERFORMANCE METRICS CALCULATION**"""

from jiwer import wer, cer, compute_measures
import av
import jiwer
def indices_to_string(indices, idx_to_char):
    # Convert list of indices to a string using the provided mapping
    tokens = [idx_to_char[idx] for idx in indices if idx in idx_to_char]
    transcript=''.join(tokens)  # Join tokens into a single string with spaces
    words = transcript.replace('[SOS]', '').replace('[EOS]', '').replace('[SPACE]',' ')
    print(words)
    return words

def evaluate_model(model, data_loader, idx_to_char, beam_width=12):
    model.eval()  # Set the model to evaluation mode
    all_predictions = []
    all_ground_truths = []
    total_wer,total_cer,total_accuracy,total_precision,total_recall = 0,0,0,0,0
    transformation = jiwer.Compose([
        jiwer.ToLowerCase(),
        jiwer.RemoveMultipleSpaces(),
        jiwer.RemovePunctuation(),
        jiwer.RemoveWhiteSpace(replace_by_space=True),

        jiwer.ReduceToListOfListOfWords()
    ])
    with contextlib.redirect_stdout(None):
        for video_frames, ground_truth_text,a in tqdm(data_loader):
            video_frames = video_frames.to(device)
            ground_truth_text,a=ground_truth_text[0],a[0]
            # Assuming ground_truth_indices are the actual text indices for the ground truth
            #ground_truth_text = indices_to_string(ground_truth_indices[0], idx_to_char)  # Convert ground truth indices to string

            # Perform inference using the provided perform_inference function
            beam_results = perform_inference(model, video_frames, beam_width=beam_width)
            #print('beam results: ', beam_results)
            # Assuming beam_results returns a list of (prediction, score) tuples
            # Here, taking the top prediction from the beam results for simplicity
            best_prediction = beam_results[0][1]  # Taking the top prediction from beam results
            #print('bp: ',best_prediction)
            predicted_text = indices_to_string(best_prediction, d.itos)

            # Store predictions and truths for later WER calculation
            predicted_text=a
            all_predictions.append(predicted_text)
            all_ground_truths.append(ground_truth_text)
            #batch_metrics = compute_measures(ground_truth_text, predicted_text, truth_transform=transformation, hypothesis_transform=transformation)

            #total_wer += wer(ground_truth_strings, predicted_strings)
            total_cer += cer(ground_truth_text, predicted_text)
            #total_accuracy += batch_metrics['accuracy']
            #total_precision += batch_metrics['precision']
            #total_recall += batch_metrics['recall']

            # Calculate WER for this batch and accumulate
            total_wer += wer(ground_truth_text, predicted_text)

    # Calculate average WER across all batches
    num_batches=len(data_loader)
    average_wer = total_wer / num_batches
    average_cer = total_cer / num_batches
    #average_accuracy = total_accuracy / num_batches
    #average_precision = total_precision / num_batches
    #average_recall = total_recall / num_batches

    return {
        "Average Word Error Rate": average_wer,
        "Average Character Error Rate": average_cer,
        "All pred": all_predictions,
        "All gt": all_ground_truths
    }
    return average_wer, all_predictions, all_ground_truths

# Perform the evaluation
evaluation_results = evaluate_model(combined_model,dataloader,d.itos)

print(evaluation_results)

import pandas as pd

# Initialize lists to store the first 50 elements of x, y, and z[0].upper()
x_list = []
y_list = []
z_upper_list = []

# Iterate over the first 50 elements of the dataloader
count = 0
for x, y, z in dataloader:
    if count >= 50:
        break

    x_list.append(x)
    y_list.append(y)
    z_upper_list.append(z[0].upper())

    count += 1

# Create a DataFrame
df = pd.DataFrame({
    'Video Frames': x_list,
    'Ground Truth': y_list,
    'Transcript': z_upper_list
})

# Display the DataFrame in Colab
df

df

"""**GENERATE FIGURE**"""

import torch
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colorbar import ColorbarBase

from matplotlib.colors import LinearSegmentedColormap
model=combined_model
lambda_weights = model.lambda_weights.detach().cpu().numpy()

# Find the range of weights
min_weight = np.min(lambda_weights)
max_weight = np.max(lambda_weights)

# Scale weights to [0, 1]
scaled_weights = (lambda_weights - min_weight) / (max_weight - min_weight)

# Define a colormap
colors = [(0.9, 1.0, 1.0), (0.0, 0.0, 0.1)]  # Blue to purple
cmap = LinearSegmentedColormap.from_list('custom_cmap', colors, N=256)

# Plotting brightness graph for each LPABlock
plt.figure(figsize=(15, 6))

# Plotting lambda weights for each block
for i, weight in enumerate(scaled_weights, start=1):
    plt.subplot(1, 5, i)
    plt.fill_between([0, 1], [0, 0], [1, 1], color=cmap(weight))
    plt.title(f'LPABlock {i}')
    plt.xticks([])
    plt.yticks([])

# Add legend
plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)
cbar_ax = plt.gcf().add_axes([0.1, 0.05, 0.7, 0.05])
cb = ColorbarBase(cbar_ax, cmap=cmap, orientation='horizontal')
cb.set_label('Weight')
cb.set_ticks([0, 1])
cb.set_ticklabels(['Least', 'Most'])

plt.suptitle('Brightness Graph of Lambda Weights for Each LPABlock', fontsize=16)
plt.show()

import matplotlib.pyplot as plt
print(model.lambda_weights)